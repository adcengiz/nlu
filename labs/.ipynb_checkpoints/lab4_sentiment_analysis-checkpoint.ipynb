{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis (Lab 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Alex Wang\"\n",
    "__version__ = \"DSGA 1012, NYU, Spring 2019 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll go through the process of processing a dataset, designing features, fitting a model on the feature data (sort of), and evaluate on a held-out test set. For the **bonus**, we'll have a friendly competition to see who can get the highest performance on a held out test set from a different distribution, so think throughout about how to improve and generalize our model's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the Stanford Sentiment Treebank. Download it from here: [the train/dev/test Stanford Sentiment Treebank distribution](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip), unzip it, and put the resulting folder in the same directory as this notebook. (If you want to put it somewhere else, change `sst_home` below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trees/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ebfae6771421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sst_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msst_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sst_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msst_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sst_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msst_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ebfae6771421>\u001b[0m in \u001b[0;36mload_sst_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trees/train.txt'"
     ]
    }
   ],
   "source": [
    "sst_home = 'trees'\n",
    "\n",
    "def load_sst_data(path):\n",
    "    # Let's do 2-way positive/negative classification instead of 5-way\n",
    "    EASY_LABEL_MAP = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    \n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = EASY_LABEL_MAP[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    return data\n",
    "     \n",
    "train = load_sst_data('trees/train.txt')\n",
    "val = load_sst_data('/dev.txt')\n",
    "test = load_sst_data('/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to build some sort of feature representation of our data. One of the simplest things we can do is to represent each sentence as a bag of its words. As part of determining what constitutes a work (or \"token\"), we'll have to choose how to tokenize the data. Let's do the simplest thing for now and just split on whitespace. More sophisticated methods might use a tokenizer from an outside library, such as NLTK or SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    ''' Bare-bones tokenization '''\n",
    "    return string.split()\n",
    "\n",
    "def extract_feats(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "                         \n",
    "    # Extract vocabulary\n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]: # assume first dataset is training set\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    vocabulary = set(word_counter.keys())\n",
    "\n",
    "    features = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            #Extract features (by name) for one example:\n",
    "            word2count = collections.Counter(tokenize(example['text']))\n",
    "            for word, count in word2count.items():\n",
    "                if word in vocabulary:\n",
    "                    example[\"features\"][word] = min(count, 1) # these are *binary* features\n",
    "            \n",
    "            features.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feat2idx = dict(zip(features, range(len(features))))\n",
    "    idx2feat = {v: k for k, v in feat2idx.items()}\n",
    "    dim = len(feat2idx)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['input'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['input'][feat2idx[feature]] = example['features'][feature]\n",
    "    return idx2feat\n",
    "    \n",
    "idx2feat = extract_feats([train, val, test]) # adds the features as a key in each example dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a classifier for this dataset. Because we haven't talked about optimization yet, we’ll use the LogisticRegression class from scikit-learn out-of-the-box.\n",
    "\n",
    "You might need to install scikit-learn via the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn the \"best\" parameters for our model based on the training data, we use scikit-learn’s fit method. Inside this method, the parameters are according to some loss function (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train = [x['input'] for x in train]\n",
    "y_train = [y['label'] for y in train]\n",
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained sentiment analysis model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Model and Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do? Let's define a function to see our model's accuracy on some data split and see how well we fit the training data. We'll make use of the `model.predict()` interface for generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(inputs, targs, model):\n",
    "    preds = model.predict(inputs)\n",
    "    return accuracy_score(preds, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 98.367\n"
     ]
    }
   ],
   "source": [
    "X_train = [x['input'] for x in train]\n",
    "y_train = [y['label'] for y in train]\n",
    "train_acc = evaluate(X_train, y_train, log_model)\n",
    "print(\"Train acc: %.3f\" % (100 * train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, 98% accuracy. How well do we do on held-out data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev acc: 77.982\n"
     ]
    }
   ],
   "source": [
    "X_dev = [x['input'] for x in val]\n",
    "y_dev = [y['label'] for y in val]\n",
    "dev_acc = evaluate(X_dev, y_dev, log_model)\n",
    "print(\"Dev acc: %.3f\" % (100 * dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a big drop, ~20 accuracy, on held-out data, so we overfit the training data. We can go back and revise our approach (e.g. by playing around with the different parameters for the [logistic regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) and re-fitting on the training data, and then see how well we do on the held-out validation data.\n",
    "\n",
    "By doing this, however, we'll be fitting to the validation data. At some point, we'll want to evaluate one completely new data. Which is what the test split is for. The test split should be used as sparingly as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 79.791\n"
     ]
    }
   ],
   "source": [
    "X_test = [x['input'] for x in test]\n",
    "y_test = [y['label'] for y in test]\n",
    "test_acc = evaluate(X_test, y_test, log_model)\n",
    "print(\"Test acc: %.3f\" % (100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remaining time, try to maximize your model's performance on the test split without evaluating on it (until the end of class). How you go about that is completely open (feature engineering, modeling, optimization, etc.), but do not use pretrained models or libraries outside the ones we have used today. You should work on this by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !?! ~ ~ * * BONUS * * ~ ~ !?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been evaluating on data drawn roughly from the same data distribution. How do our models fare if we move out-of-distribution? \n",
    "\n",
    "Once the data is distributed, the following function reformats it in the same form as our SST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mystery_data(path):\n",
    "    \n",
    "    pos_data, neg_data = [], []\n",
    "    all_files = []\n",
    "    _limit = 250\n",
    "    \n",
    "    for dirpath, dirnames, files in os.walk(path):\n",
    "        for name in files:\n",
    "            all_files.append(os.path.join(dirpath, name))\n",
    "            \n",
    "            \n",
    "    for file_path in all_files:\n",
    "        if '/neg' in file_path and len(neg_data) <= _limit:\n",
    "            example = {}\n",
    "            with open(file_path, 'r') as myfile:\n",
    "                example['text'] = myfile.read().replace('\\n', '')\n",
    "            example['label'] = 0\n",
    "            neg_data.append(example)\n",
    "            \n",
    "        if '/pos' in file_path and len(pos_data) <= _limit:\n",
    "            example = {}\n",
    "            with open(file_path, 'r') as myfile:\n",
    "                example['text'] = myfile.read().replace('\\n', '')\n",
    "            example['label'] = 1\n",
    "            pos_data.append(example)\n",
    "    data = neg_data + pos_data\n",
    "\n",
    "    return data\n",
    "\n",
    "            \n",
    "mystery_test = load_mystery_data('path/to/data')\n",
    "idx2feat = extract_feats([train, mystery_test]) # adds the features as a key in each example dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
