{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sippycup-small.jpg\" align=\"left\" style=\"padding-right: 30px\"/>\n",
    "\n",
    "<h1 style=\"line-height: 125%\">\n",
    "  SippyCup<br />\n",
    "  Unit 1: Natural language arithmetic\n",
    "</h1>\n",
    "\n",
    "<p>\n",
    "  Nikita Nangia\n",
    "  <br>\n",
    "  Material taken from <a href=\"http://nlp.stanford.edu/~wcmac/\">Bill MacCartney</a>'s SippyCup tutorial.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Unit 1 of the SippyCup codelab. This is one of four parts of a more comprehensive overview of SippyCup. You can find the rest of the tutorial on the [SippyCup github page](https://github.com/wcmac/sippycup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data\n",
    "## Example inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arithmetic domain is simple and straightforward, and for now our goals are strictly pedagogical.  So a small, artificial sample of inputs will suffice.  Here's a sample of 17 inputs borrowed from the [companion code][] to [Liang & Potts 2015][].\n",
    "\n",
    "  [companion code]: https://github.com/cgpotts/annualreview-complearning\n",
    "  [Liang & Potts 2015]: http://www.annualreviews.org/doi/pdf/10.1146/annurev-linguist-030514-125312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"one plus one\",\n",
    "    \"one plus two\",\n",
    "    \"one plus three\",\n",
    "    \"two plus two\",\n",
    "    \"two plus three\",\n",
    "    \"three plus one\",\n",
    "    \"three plus minus two\",\n",
    "    \"two plus two\",\n",
    "    \"three minus two\",\n",
    "    \"minus three minus two\",\n",
    "    \"two times two\",\n",
    "    \"two times three\",\n",
    "    \"three plus three minus two\",\n",
    "    \"minus three\",\n",
    "    \"three plus two\",\n",
    "    \"two times two plus three\",\n",
    "    \"minus four\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, even for the arithmetic domain, this sample is quite limited in scope.  It uses only the integers one through four, and it uses only a small number of arithmetic operations.  The [exercises](#arithmetic-exercises) at the end of this unit will challenge you to extend the scope of the problem in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic representation <a id=\"arithmetic-semantic-representation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having collected a sample of inputs, the next order of business is to choose a good semantic representation.  After all, the semantic representation is the desired *output* of the semantic parsing system, so the choice of representation will drive many other decisions.\n",
    "\n",
    "**Our semantic representation should be machine-readable, unambiguous, and easily executable**.  For the domain of natural language arithmetic, a natural choice is to use [binary expression trees][], represented in Python by nested tuples. All of the following are valid semantic representations:\n",
    "\n",
    "  [binary expression trees]: http://en.wikipedia.org/wiki/Binary_expression_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sems = [\n",
    "    ('+', 1, 1),                # one plus one\n",
    "    ('-', ('~', 3), 2),         # minus three minus two\n",
    "    ('-', ('+', 3, 3), 2),      # three plus three minus two\n",
    "    ('+', ('*', 2, 2), 3),      # two times two plus three\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to implement an executor which actually performs the arithmetic calculations described by these semantic representations to return a denotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = {\n",
    "    '~': lambda x: -x,\n",
    "    '+': lambda x, y: x + y,\n",
    "    '-': lambda x, y: x - y,\n",
    "    '*': lambda x, y: x * y,\n",
    "}\n",
    "\n",
    "def execute(sem):\n",
    "    if isinstance(sem, tuple):\n",
    "        op = ops[sem[0]]\n",
    "        args = [execute(arg) for arg in sem[1:]]\n",
    "        return op(*args)\n",
    "    else:\n",
    "        return sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the executor is simple, straightforward, deterministic, and doesn't rely on any linguistic knowledge.  This is a sign that we've chosen a good semantic representation!\n",
    "\n",
    "Let's see the executor in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('+', 1, 1) = 2\n",
      "('-', ('~', 3), 2) = -5\n",
      "('-', ('+', 3, 3), 2) = 4\n",
      "('+', ('*', 2, 2), 3) = 7\n"
     ]
    }
   ],
   "source": [
    "for sem in sems:\n",
    "    print('{} = {}'.format(sem, execute(sem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data <a id=\"arithmetic-example-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected a sample of inputs and defined a semantic representation, we can construct a set of *examples* which pair inputs with their target outputs.\n",
    "The SippyCup codebase defines (in [`example.py`](./example.py)) a simple container class called `Example`.  By importing this class, we can define some examples to guide our development.\n",
    "\n",
    "<!-- TODO: consider moving all imports to a block at the end of Unit 0. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example import Example\n",
    "\n",
    "arithmetic_examples = [\n",
    "    Example(input=\"one plus one\", semantics=('+', 1, 1), denotation=2),\n",
    "    Example(input=\"one plus two\", semantics=('+', 1, 2), denotation=3),\n",
    "    Example(input=\"one plus three\", semantics=('+', 1, 3), denotation=4),\n",
    "    Example(input=\"two plus two\", semantics=('+', 2, 2), denotation=4),\n",
    "    Example(input=\"two plus three\", semantics=('+', 2, 3), denotation=5),\n",
    "    Example(input=\"three plus one\", semantics=('+', 3, 1), denotation=4),\n",
    "    Example(input=\"three plus minus two\", semantics=('+', 3, ('~', 2)), denotation=1),\n",
    "    Example(input=\"two plus two\", semantics=('+', 2, 2), denotation=4),\n",
    "    Example(input=\"three minus two\", semantics=('-', 3, 2), denotation=1),\n",
    "    Example(input=\"minus three minus two\", semantics=('-', ('~', 3), 2), denotation=-5),\n",
    "    Example(input=\"two times two\", semantics=('*', 2, 2), denotation=4),\n",
    "    Example(input=\"two times three\", semantics=('*', 2, 3), denotation=6),\n",
    "    Example(input=\"three plus three minus two\", semantics=('-', ('+', 3, 3), 2), denotation=4),\n",
    "    Example(input=\"minus three\", semantics=('~', 3), denotation=-3),\n",
    "    Example(input=\"three plus two\", semantics=('+', 3, 2), denotation=5),\n",
    "    Example(input=\"two times two plus three\", semantics=('+', ('*', 2, 2), 3), denotation=7),\n",
    "    Example(input=\"minus four\", semantics=('~', 4), denotation=-4),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: Consider whether to divide these into train examples and test examples. -->\n",
    "\n",
    "Note that for examples with syntactically ambiguous inputs, our target semantics and denotations reflects a specific choice about how to resolve the ambiguity which accords with the standard [order of operations][].\n",
    "<!-- TODO: rewrite? -->\n",
    "\n",
    "  [order of operations]: http://en.wikipedia.org/wiki/Order_of_operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Syntactic Parsing\n",
    "### Define a CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a valid parse tree over an input, we need to know the space of possibilities.  This is the role of the *grammar*, which in SippyCup is a [context-free grammar][] (CFG).  The grammar contains a collection of *rules*, each of which specifies a valid *local subtree*, consisting of a parent and its immediate children.\n",
    "\n",
    "  [context-free grammar]: http://en.wikipedia.org/wiki/Context-free_grammar\n",
    "\n",
    "Here's a simple Python class for representing grammar rules.  You can ignore `sem` for now â€” we're only concerned with syntax at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the rules of the cfg\n",
    "class Rule:\n",
    "    def __init__(self, lhs, rhs, sem=None):\n",
    "        self.lhs = lhs\n",
    "        self.rhs = tuple(rhs.split()) if isinstance(rhs, str) else rhs\n",
    "        self.sem = sem\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Rule' + str((self.lhs, ' '.join(self.rhs), self.sem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write down a few grammar rules for the arithmetic domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeral_rules = [\n",
    "    Rule('$E', 'one'),\n",
    "    Rule('$E', 'two'),\n",
    "    Rule('$E', 'three'),\n",
    "    Rule('$E', 'four'),\n",
    "]\n",
    "\n",
    "operator_rules = [\n",
    "    Rule('$UnOp', 'minus'),\n",
    "    Rule('$BinOp', 'plus'),\n",
    "    Rule('$BinOp', 'minus'),\n",
    "    Rule('$BinOp', 'times'),\n",
    "]\n",
    "\n",
    "compositional_rules = [\n",
    "    Rule('$E', '$UnOp $E'),\n",
    "    Rule('$EBO', '$E $BinOp'),  # binarized rule\n",
    "    Rule('$E', '$EBO $E'),      # binarized rule\n",
    "]\n",
    "\n",
    "def arithmetic_rules():\n",
    "    return numeral_rules + operator_rules + compositional_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these rules are in [Chomsky normal form][]. Roughly, CNF requires that every grammar rule have one of two forms:\n",
    "\n",
    "  [Chomsky normal form]: http://en.wikipedia.org/wiki/Chomsky_normal_form\n",
    "\n",
    "- In a *unary lexical rule*, the RHS must consist of exactly one word (terminal).\n",
    "- In a *binary compositional rule*, the RHS must consist of exactly two categories (non-terminals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions which will help us ensure that our __grammars are in CNF.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cat(label):\n",
    "    return label.startswith('$')\n",
    "\n",
    "def is_lexical(rule):\n",
    "    return all([not is_cat(rhsi) for rhsi in rule.rhs])\n",
    "\n",
    "def is_binary(rule):\n",
    "    return len(rule.rhs) == 2 and is_cat(rule.rhs[0]) and is_cat(rule.rhs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a `Grammar` class to hold a collection of rules.  We'll store the rules in maps indexed by their right-hand sides, which will facilitate lookup during parsing.  Ignore the `parse_input()` method for now â€”Â we'll define it shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Grammar:\n",
    "    def __init__(self, rules=[]):\n",
    "        self.lexical_rules = defaultdict(list)\n",
    "        self.binary_rules = defaultdict(list)\n",
    "        for rule in rules:\n",
    "            add_rule(self, rule)\n",
    "        print('Created grammar with %d rules.' % len(rules))\n",
    "        \n",
    "    def parse_input(self, input):\n",
    "        \"\"\"Returns a list of parses for the given input.\"\"\"\n",
    "        return parse_input(self, input)  # defined later\n",
    "\n",
    "def add_rule(grammar, rule):\n",
    "    if is_lexical(rule):\n",
    "        grammar.lexical_rules[rule.rhs].append(rule)\n",
    "    elif is_binary(rule):\n",
    "        grammar.binary_rules[rule.rhs].append(rule)\n",
    "    else:\n",
    "        raise Exception('Cannot accept rule: %s' % rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a grammar using the rules we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 11 rules.\n"
     ]
    }
   ],
   "source": [
    "arithmetic_grammar = Grammar(arithmetic_rules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a grammar.  Now we need to implement a parsing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerate all valid parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question is, given a grammar and a specific input, how can we find the set of parses for the input which are allowed by the grammar?\n",
    "<!-- TODO: Because the space of possible parses is, in general, exponential, ... -->\n",
    "To solve this problem, we're going to use a variant of the [CYK algorithm][], which is an example of a [chart parsing][] algorithm, and more broadly, of [dynamic programming][].\n",
    "\n",
    "  [CYK algorithm]: http://en.wikipedia.org/wiki/CYK_algorithm\n",
    "  [chart parsing]: http://en.wikipedia.org/wiki/Chart_parser\n",
    "  [dynamic programming]: http://en.wikipedia.org/wiki/Dynamic_programming\n",
    "\n",
    "<!--Chart parsing relies on a data structure known as the *chart*, which has one entry (known as a *cell*) for every possible span in the input.  Spans are identified by a pair of token indices (*i*, *j*), where *i* is the (0-based) index of the leftmost token of the span, and *j* is one greater than the index of the rightmost token of the span.  It follows that *j* â€“ *i* is equal to the length of the span.  For example, if the input is \"one plus two\", then span (0, 1) is \"one\", span (1, 3) is \"plus two\", and span (0, 3) is \"one plus two\".  The chart cell for each span holds a list of possible parses for that span.-->\n",
    "\n",
    "<!--The chart parsing algorithm works like this:-->\n",
    "\n",
    "<!--- Split the input into a sequence of tokens.\n",
    "- Construct a chart, which maps from each span of the input to a list of possible parses for that span.\n",
    "- Iterate over all possible spans, working bottom-up, from smaller spans to larger spans.\n",
    "- For each span, and for each grammar rule, if the rule lets you build a parse for the span, add it to the chart.-->\n",
    "\n",
    "<!--Here's how to express the algorithm in Python.  It's surprisingly simple!-->\n",
    "\n",
    "The CYK Algorithm gives you all the possible parses, given a __grammar__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def parse_input(grammar, input):\n",
    "    \"\"\"Returns a list of all parses for input using grammar.\"\"\"\n",
    "    tokens = input.split()\n",
    "    chart = defaultdict(list)  # map from span (i, j) to list of parses\n",
    "    for j in range(1, len(tokens) + 1):\n",
    "        for i in range(j - 1, -1, -1):\n",
    "            apply_lexical_rules(grammar, chart, tokens, i, j)\n",
    "            apply_binary_rules(grammar, chart, i, j)\n",
    "    return chart[(0, len(tokens))]  # return all parses for full span\n",
    "\n",
    "def apply_lexical_rules(grammar, chart, tokens, i, j):\n",
    "    \"\"\"Add parses to span (i, j) in chart by applying lexical rules from grammar to tokens.\"\"\"\n",
    "    for rule in grammar.lexical_rules[tuple(tokens[i:j])]:\n",
    "        chart[(i, j)].append(Parse(rule, tokens[i:j]))\n",
    "\n",
    "def apply_binary_rules(grammar, chart, i, j):\n",
    "    \"\"\"Add parses to span (i, j) in chart by applying binary rules from grammar.\"\"\"\n",
    "    for k in range(i + 1, j):  # all ways of splitting the span into two subspans\n",
    "        for parse_1, parse_2 in product(chart[(i, k)], chart[(k, j)]):\n",
    "            for rule in grammar.binary_rules[(parse_1.rule.lhs, parse_2.rule.lhs)]:\n",
    "                chart[(i, j)].append(Parse(rule, [parse_1, parse_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's define the `Parse` class. It's a simple container class which stores the `Rule` used to build the parse and the children to which the rule was applied.  If the rule was a lexical rule, the children are just tokens; if it was a compositional rule, the children are other `Parse`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse:\n",
    "    def __init__(self, rule, children):\n",
    "        self.rule = rule\n",
    "        self.children = tuple(children[:])\n",
    "        self.semantics = compute_semantics(self)  # Ignore this for now -- we'll use it later.\n",
    "        self.score = float('NaN')                 # Ditto.\n",
    "        self.denotation = None                    # Ditto.\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(%s %s)' % (self.rule.lhs, ' '.join([str(c) for c in self.children]))\n",
    "    \n",
    "def compute_semantics(parse):\n",
    "    return None                                   # We'll redefine this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "We're finally ready to do some parsing!  Let's see our grammar in action, by applying it to the set of 17 examples we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 11 rules.\n",
      "\n",
      "input            one plus one\n",
      "parse 0          ($E ($EBO ($E one) ($BinOp plus)) ($E one))\n",
      "\n",
      "input            one plus two\n",
      "parse 0          ($E ($EBO ($E one) ($BinOp plus)) ($E two))\n",
      "\n",
      "input            one plus three\n",
      "parse 0          ($E ($EBO ($E one) ($BinOp plus)) ($E three))\n",
      "\n",
      "input            two plus two\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp plus)) ($E two))\n",
      "\n",
      "input            two plus three\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp plus)) ($E three))\n",
      "\n",
      "input            three plus one\n",
      "parse 0          ($E ($EBO ($E three) ($BinOp plus)) ($E one))\n",
      "\n",
      "input            three plus minus two\n",
      "parse 0          ($E ($EBO ($E three) ($BinOp plus)) ($E ($UnOp minus) ($E two)))\n",
      "\n",
      "input            two plus two\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp plus)) ($E two))\n",
      "\n",
      "input            three minus two\n",
      "parse 0          ($E ($EBO ($E three) ($BinOp minus)) ($E two))\n",
      "\n",
      "input            minus three minus two\n",
      "parse 0          ($E ($UnOp minus) ($E ($EBO ($E three) ($BinOp minus)) ($E two)))\n",
      "parse 1          ($E ($EBO ($E ($UnOp minus) ($E three)) ($BinOp minus)) ($E two))\n",
      "\n",
      "input            two times two\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp times)) ($E two))\n",
      "\n",
      "input            two times three\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp times)) ($E three))\n",
      "\n",
      "input            three plus three minus two\n",
      "parse 0          ($E ($EBO ($E three) ($BinOp plus)) ($E ($EBO ($E three) ($BinOp minus)) ($E two)))\n",
      "parse 1          ($E ($EBO ($E ($EBO ($E three) ($BinOp plus)) ($E three)) ($BinOp minus)) ($E two))\n",
      "\n",
      "input            minus three\n",
      "parse 0          ($E ($UnOp minus) ($E three))\n",
      "\n",
      "input            three plus two\n",
      "parse 0          ($E ($EBO ($E three) ($BinOp plus)) ($E two))\n",
      "\n",
      "input            two times two plus three\n",
      "parse 0          ($E ($EBO ($E two) ($BinOp times)) ($E ($EBO ($E two) ($BinOp plus)) ($E three)))\n",
      "parse 1          ($E ($EBO ($E ($EBO ($E two) ($BinOp times)) ($E two)) ($BinOp plus)) ($E three))\n",
      "\n",
      "input            minus four\n",
      "parse 0          ($E ($UnOp minus) ($E four))\n"
     ]
    }
   ],
   "source": [
    "arithmetic_grammar = Grammar(arithmetic_rules())\n",
    "for example in arithmetic_examples:\n",
    "    parses = parse_input(arithmetic_grammar, example.input)\n",
    "    print()\n",
    "    print('%-16s %s' % ('input', example.input))\n",
    "    for idx, parse in enumerate(parses):\n",
    "        print('%-16s %s' % ('parse %d' % idx, parse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that all 17 examples were successfully parsed.  Moreover, the examples which exhibit ambiguity get multiple parses, exactly as expected.\n",
    "\n",
    "Syntactic parsing provides the spine around which we'll build our semantic parsing system.  But now we need to add semantics.\n",
    "<!-- TODO: reword? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Adding semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to bring semantics into the picture.  Given a parse tree, we'd like to attach a semantic representation to every node in the tree.\n",
    "The semantics are determined directly by the words: the semantics for \"one\" is `1`, the semantics for \"plus\" is `+`, and so on. This is province of [lexical semantics][].\n",
    "\n",
    "  [lexical semantics]: http://en.wikipedia.org/wiki/Lexical_semantics\n",
    "\n",
    "As we work our way up the tree, the semantic representation for each internal node is computed from the semantics of its children, in a manner that depends on the rule that was used to combine them.  This is the province of *compositional semantics*, and it hinges on the [principle of compositionality][] (often attributed to [Gottlob Frege][]).\n",
    "\n",
    "  [Principle of compositionality]: http://en.wikipedia.org/wiki/Principle_of_compositionality\n",
    "  [Gottlob Frege]: http://en.wikipedia.org/wiki/Gottlob_Frege\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; margin: 20px 100px; padding: 10px 20px 20px 20px; background-color: #eeffdd; border-style: solid; border-color: #bbccaa; border-width: 5px\">\n",
    "<h3>The principle of compositionality</h3>\n",
    "<p>The meaning of a compound expression is a function of the meanings of its parts and the manner of their combination.</p>\n",
    "</div>\n",
    "\n",
    "<!-- Now recall that the rules of our grammar specify valid local subtrees from which we can construct parse trees.  Just as we've added semantic attachments to our parse trees, we'll add semantic attachments to the rules of our grammar. The semantic attachment to a rule specifies how to construct the semantics for the parent (LHS) category.  For a lexical rule, the semantic attachment is simply a semantic representation (or a fragment thereof).  For a compositional rule, the semantic attachment is a function which takes the semantics of the children as input and returns the semantics for the parent. -->\n",
    "\n",
    "Let's see how this looks in Python.  We need to redefine our rules to include semantic attachments.  For *lexical* rules, the semantic attachments directly specify semantic representations (or fragments thereof):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeral_rules = [\n",
    "    Rule('$E', 'one', 1),\n",
    "    Rule('$E', 'two', 2),\n",
    "    Rule('$E', 'three', 3),\n",
    "    Rule('$E', 'four', 4),\n",
    "]\n",
    "\n",
    "operator_rules = [\n",
    "    Rule('$UnOp', 'minus', '~'),\n",
    "    Rule('$BinOp', 'plus', '+'),\n",
    "    Rule('$BinOp', 'minus', '-'),\n",
    "    Rule('$BinOp', 'times', '*'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *compositional* rules, the semantic attachments are functions which specify how to construct the semantics of the parent from the semantics of the children.  We'll define these functions using Python's `lambda` syntax, and we'll establish the convention that these lambda functions always have a single parameter called `sems`, which will contain a list of the semantic representations of the children on the RHS of the rule.\n",
    "\n",
    "For example, consider the rule which specifies how to combine a unary operator with its argument.  We can define its semantic attachment like this:\n",
    "\n",
    "    Rule('$E', '$UnOp $E', lambda sems: (sems[0], sems[1]))\n",
    "\n",
    "Now, `sems[0]` refers to the semantics of the child `$UnOp`,\n",
    "and `sems[1]` refers to the semantics of the child `$E`.\n",
    "So this semantic attachment says: take the semantics of the child `$UnOp` and the semantics of the child `$E`, form a pair from them, and return that pair as the semantics for the parent `$E`.\n",
    "\n",
    "For the other two rules, first we combine the semantics of the left child `$E` with the semantics of `$BinOp`, forming a semantic representation for `$EBO` which is an \"incomplete\" tuple:\n",
    "\n",
    "    Rule('$EBO', '$E $BinOp', lambda sems: (sems[1], sems[0]))\n",
    "\n",
    "This semantic representation is incomplete in the sense that it combines a binary operator with a single argument.\n",
    "\n",
    "Then we follow through by combining the \"incomplete\" semantics for `$EBO` with the semantics for the right child `$E` to yield the semantic representation for the parent `$E`:\n",
    "\n",
    "    Rule('$E', '$EBO $E', lambda sems: (sems[0][0], sems[0][1], sems[1]))\n",
    "\n",
    "Putting everything together, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositional_rules = [\n",
    "    Rule('$E', '$UnOp $E', lambda sems: (sems[0], sems[1])),\n",
    "    Rule('$EBO', '$E $BinOp', lambda sems: (sems[1], sems[0])),\n",
    "    Rule('$E', '$EBO $E', lambda sems: (sems[0][0], sems[0][1], sems[1])),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now that we have rules for composition, let's write a function to construct the smenatics for a parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "\n",
    "def compute_semantics(parse):\n",
    "    if is_lexical(parse.rule) or not isinstance(parse.rule.sem, FunctionType):\n",
    "        return parse.rule.sem\n",
    "    else:\n",
    "        return parse.rule.sem([child.semantics for child in parse.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voila!\n",
    "\n",
    "We now have all the pieces we need to do semantic parsing.  Let's test that by parsing \"two times two plus three\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 11 rules.\n",
      "\n",
      "($E ($EBO ($E two) ($BinOp times)) ($E ($EBO ($E two) ($BinOp plus)) ($E three)))\n",
      "('*', 2, ('+', 2, 3))\n",
      "\n",
      "($E ($EBO ($E ($EBO ($E two) ($BinOp times)) ($E two)) ($BinOp plus)) ($E three))\n",
      "('+', ('*', 2, 2), 3)\n"
     ]
    }
   ],
   "source": [
    "arithmetic_grammar = Grammar(arithmetic_rules())\n",
    "parses = parse_input(arithmetic_grammar, \"two times two plus three\")\n",
    "for parse in parses:\n",
    "    print()\n",
    "    print(parse)\n",
    "    print(parse.semantics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, it looks like it's working!\n",
    "\n",
    "Next we'd like to evaluate the performance of our semantic grammar on our whole dataset.  The SippyCup codebase includes a number of helper functions (in [`experiment.py`](./experiment.py)) to facilitate empirical evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 11 rules.\n",
      "================================================================================\n",
      "Evaluating on 17 examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              one plus one\n",
      "target semantics                   ('+', 1, 1)\n",
      "target denotation                  2\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 1, 1)\n",
      "               denotation      +   2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              one plus two\n",
      "target semantics                   ('+', 1, 2)\n",
      "target denotation                  3\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 1, 2)\n",
      "               denotation      +   3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              one plus three\n",
      "target semantics                   ('+', 1, 3)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 1, 3)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two plus two\n",
      "target semantics                   ('+', 2, 2)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 2, 2)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two plus three\n",
      "target semantics                   ('+', 2, 3)\n",
      "target denotation                  5\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 2, 3)\n",
      "               denotation      +   5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              three plus one\n",
      "target semantics                   ('+', 3, 1)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 3, 1)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              three plus minus two\n",
      "target semantics                   ('+', 3, ('~', 2))\n",
      "target denotation                  1\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 3, ('~', 2))\n",
      "               denotation      +   1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two plus two\n",
      "target semantics                   ('+', 2, 2)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 2, 2)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              three minus two\n",
      "target semantics                   ('-', 3, 2)\n",
      "target denotation                  1\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('-', 3, 2)\n",
      "               denotation      +   1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              minus three minus two\n",
      "target semantics                   ('-', ('~', 3), 2)\n",
      "target denotation                  -5\n",
      "\n",
      "semantics accuracy                 0\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                0\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   2\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       -   ('~', ('-', 3, 2))\n",
      "               denotation      -   -1\n",
      "1      0.000   semantics       +   ('-', ('~', 3), 2)\n",
      "               denotation      +   -5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two times two\n",
      "target semantics                   ('*', 2, 2)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('*', 2, 2)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two times three\n",
      "target semantics                   ('*', 2, 3)\n",
      "target denotation                  6\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('*', 2, 3)\n",
      "               denotation      +   6\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              three plus three minus two\n",
      "target semantics                   ('-', ('+', 3, 3), 2)\n",
      "target denotation                  4\n",
      "\n",
      "semantics accuracy                 0\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   2\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       -   ('+', 3, ('-', 3, 2))\n",
      "               denotation      +   4\n",
      "1      0.000   semantics       +   ('-', ('+', 3, 3), 2)\n",
      "               denotation      +   4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              minus three\n",
      "target semantics                   ('~', 3)\n",
      "target denotation                  -3\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('~', 3)\n",
      "               denotation      +   -3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              three plus two\n",
      "target semantics                   ('+', 3, 2)\n",
      "target denotation                  5\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('+', 3, 2)\n",
      "               denotation      +   5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              two times two plus three\n",
      "target semantics                   ('+', ('*', 2, 2), 3)\n",
      "target denotation                  7\n",
      "\n",
      "semantics accuracy                 0\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                0\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   2\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       -   ('*', 2, ('+', 2, 3))\n",
      "               denotation      -   10\n",
      "1      0.000   semantics       +   ('+', ('*', 2, 2), 3)\n",
      "               denotation      +   7\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "input                              minus four\n",
      "target semantics                   ('~', 4)\n",
      "target denotation                  -4\n",
      "\n",
      "semantics accuracy                 1\n",
      "semantics oracle accuracy          1\n",
      "denotation accuracy                1\n",
      "denotation oracle accuracy         1\n",
      "number of parses                   1\n",
      "spurious ambiguity                 0\n",
      "\n",
      "0      0.000   semantics       +   ('~', 4)\n",
      "               denotation      +   -4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 17 examples:\n",
      "\n",
      "semantics accuracy                 0.824\n",
      "semantics oracle accuracy          1.000\n",
      "denotation accuracy                0.882\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.176\n",
      "spurious ambiguity                 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from experiment import evaluate_grammar\n",
    "\n",
    "arithmetic_grammar = Grammar(arithmetic_rules())\n",
    "evaluate_grammar(grammar=arithmetic_grammar, executor=execute, examples=arithmetic_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap between accuracy and oracle accuracy represents an opportunity!  If we had some way of ranking candidate parses so that correct parses were likely to appear higher in the list, then we could bring accuracy closer to oracle accuracy.  (The \"oracle\"  in question is one that magically knows the optimal ranking of candidate parses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scoring candidate parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've seen only a couple of cases where the parser found more than one parse for a given input.  **But in richer domains, with more complex grammars, it's not unusual to find tens, hundreds, or even thousands of parses for some inputs.**  However, the list of candidate parses returned by `parse_input()` appears in arbitrary order.  The first parse is not necessarily the best, and the best parse is not necessarily the first.  Therefore, we'd like to have some way of **ranking the candidates**, so that more plausible interpretations appear earlier in the list.\n",
    "\n",
    "  [order of operations]: http://en.wikipedia.org/wiki/Order_of_operations\n",
    "\n",
    "An easy way to rank candidate parses is with a linear scoring function. If $p$ is a parse, $w$ is the weight vector, and $\\phi$ is the vector of feature functions, we can write this as:\n",
    "\n",
    "  [feature functions]: http://en.wikipedia.org/wiki/Feature_(machine_learning)\n",
    "  [inner product]: http://en.wikipedia.org/wiki/Dot_product\n",
    "\n",
    "$ score(p) = \\sum_i w_i \\cdot \\phi_i(p) $\n",
    "<!-- TODO: center this? -->\n",
    "\n",
    "Finally, we sort the candidate parses by score, so that the highest-scoring parses appear first.\n",
    "\n",
    "We now have two problems:\n",
    "\n",
    "  1. **Feature engineering**: defining features which help to discriminate good parses from bad.\n",
    "  2. **Weight learning**: deciding what weight to assign to each feature.\n",
    "  \n",
    "The rest of this section will focus on weight learning.\n",
    "\n",
    "### <span style=\"color:red\">Question: Why might we want to avoid feature engineering?</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's define the scoring function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(parse=None, feature_fn=None, weights=None):\n",
    "    \"\"\"Returns the inner product of feature_fn(parse) and weights.\"\"\"\n",
    "    return sum(weights[feature] * value for feature, value in feature_fn(parse).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model that accepts generates parses and scores them,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,\n",
    "                 grammar=None,\n",
    "                 feature_fn=lambda parse: defaultdict(float),\n",
    "                 weights=defaultdict(float),\n",
    "                 executor=None):\n",
    "        self.grammar = grammar\n",
    "        self.feature_fn = feature_fn\n",
    "        self.weights = weights\n",
    "        self.executor = executor\n",
    "\n",
    "    def parse_input(self, input):\n",
    "        parses = self.grammar.parse_input(input)\n",
    "        for parse in parses:\n",
    "            if self.executor:\n",
    "                parse.denotation = self.executor(parse.semantics)\n",
    "            parse.score = score(parse, self.feature_fn, self.weights)\n",
    "        parses = sorted(parses, key=lambda parse: parse.score, reverse=True)\n",
    "        return parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the stochastic gradient descet algorithm that we will use for weight learning,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_sgd(model=None, examples=[], training_metric=None, T=10, eta=0.1, seed=None):\n",
    "    print('Running SGD learning on %d examples with training metric: %s' % (\n",
    "        len(examples), training_metric.name()))\n",
    "    if seed:\n",
    "        print('random.seed(%d)' % seed)\n",
    "        random.seed(seed)\n",
    "    model = clone_model(model)\n",
    "    for t in range(T):\n",
    "        random.shuffle(examples)\n",
    "        num_correct = 0\n",
    "        for example in examples:\n",
    "            # Parse input with current weights.\n",
    "            parses = model.parse_input(example.input)\n",
    "            # Get the highest-scoring \"good\" parse among the candidate parses.\n",
    "            good_parses = [p for p in parses if training_metric.evaluate(example, [p])]\n",
    "            if good_parses:\n",
    "                target_parse = good_parses[0]\n",
    "                # Get all (score, parse) pairs.\n",
    "                scores = [(p.score + cost(target_parse, p), p) for p in parses]\n",
    "                # Get the maximal score.\n",
    "                max_score = sorted(scores)[-1][0]\n",
    "                # Get all the candidates with the max score and choose one randomly.\n",
    "                predicted_parse = random.choice([p for s, p in scores if s == max_score])\n",
    "                if training_metric.evaluate(example, [predicted_parse]):\n",
    "                    num_correct += 1\n",
    "                update_weights(model, target_parse, predicted_parse, eta)\n",
    "        print('SGD iteration %d: train accuracy: %.3f' % (t, 1.0 * num_correct / len(examples)))\n",
    "    print_weights(model.weights)\n",
    "    return model\n",
    "\n",
    "def cost(parse_1, parse_2):\n",
    "    return 0.0 if parse_1 == parse_2 else 1.0\n",
    "\n",
    "def clone_model(model):\n",
    "    return Model(grammar=model.grammar,\n",
    "                 feature_fn=model.feature_fn,\n",
    "                 weights=defaultdict(float),  # Zero the weights.\n",
    "                 executor=model.executor)\n",
    "\n",
    "def update_weights(model, target_parse, predicted_parse, eta):\n",
    "    target_features = model.feature_fn(target_parse)\n",
    "    predicted_features = model.feature_fn(predicted_parse)\n",
    "    for f in set(target_features.keys() + predicted_features.keys()):\n",
    "        update = target_features[f] - predicted_features[f]\n",
    "        if update != 0.0:\n",
    "            # print 'update %g + %g * %g = %g\\t%s' % (\n",
    "            #     model.weights[f], eta, update, model.weights[f] + eta * update, f)\n",
    "            model.weights[f] += eta * update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Learning to score from semantics,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put `latent_sgd()` to the test, by using it to learn weights for our arithmetic model.\n",
    "We'll divide our 17 arithmetic examples into 13 training examples and 4 test examples.\n",
    "Then, we'll use the utility function `train_test()`, defined in [`experiment.py`](./experiment.py). Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 training examples, 4 test examples\n",
      "================================================================================\n",
      "Evaluating on 13 train examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 13 examples:\n",
      "\n",
      "semantics accuracy                 0.846\n",
      "semantics oracle accuracy          1.000\n",
      "denotation accuracy                0.923\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.154\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 4 test examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 4 examples:\n",
      "\n",
      "semantics accuracy                 0.750\n",
      "semantics oracle accuracy          1.000\n",
      "denotation accuracy                0.750\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.250\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Running SGD learning on 13 examples with training metric: semantics accuracy\n",
      "\n",
      "random.seed(1)\n",
      "SGD iteration 0: train accuracy: 0.846\n",
      "SGD iteration 1: train accuracy: 0.846\n",
      "SGD iteration 2: train accuracy: 0.846\n",
      "SGD iteration 3: train accuracy: 0.846\n",
      "SGD iteration 4: train accuracy: 0.846\n",
      "SGD iteration 5: train accuracy: 0.846\n",
      "SGD iteration 6: train accuracy: 1.000\n",
      "SGD iteration 7: train accuracy: 1.000\n",
      "SGD iteration 8: train accuracy: 1.000\n",
      "SGD iteration 9: train accuracy: 1.000\n",
      "\n",
      "Feature weights:\n",
      "     0.6\t('~', '-')\n",
      "     0.6\t('+', '-')\n",
      "    -0.6\t('-', '~')\n",
      "    -0.6\t('-', '+')\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 13 train examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 13 examples:\n",
      "\n",
      "semantics accuracy                 1.000\n",
      "semantics oracle accuracy          1.000\n",
      "denotation accuracy                1.000\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.154\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 4 test examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 4 examples:\n",
      "\n",
      "semantics accuracy                 0.750\n",
      "semantics oracle accuracy          1.000\n",
      "denotation accuracy                0.750\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.250\n",
      "spurious ambiguity                 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from experiment import train_test\n",
    "from metrics import SemanticsAccuracyMetric\n",
    "\n",
    "train_test(model=arithmetic_model,\n",
    "           train_examples=arithmetic_examples[:13],\n",
    "           test_examples=arithmetic_examples[13:],\n",
    "           training_metric=SemanticsAccuracyMetric(),\n",
    "           seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, while training accuracy increased from 0.846 (11 of 13 correct) to 1.000 (all 13 correct), test accuracy is only 0.750 (3 of 4 correct).  We seem to have learned *something* from the training data, but whatever we learned did not generalize to the test data.  \n",
    "\n",
    "The one error we're making on the test data involves a precedence pair that does not appear in the training data.  If you add the argument `print_examples=True` to `train_test()`, you can see which example we're getting wrong: it's `\"two times two plus three\"`.  In order to get this example right, we need to know that `*` should take precedence over `+`.  But none of the examples in the training data involved this precedence pair, so we haven't learned that.\n",
    "\n",
    "With so few training examples, it's not surprising that a test example hinges on a feature not seen in training.  Going forward, we'll strive for larger datasets which are less subject to this kind of sampling noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Learning to score from denotations,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that we need larger datasets, with more training examples.  But as we noted [above](#arithmetic-example-data), annotating examples with target semantics can be slow, expensive, and error-prone, and may require expert knowledge.  However, annotating examples with target *denotations* can often be faster, cheaper, and more reliable.\n",
    "\n",
    "### <span style=\"color:red\">Question: Why is it easier to get target denotations rather than target sematics? </span>\n",
    "\n",
    "<!-- The arithmetic domain illustrates this nicely.  Writing down the target semantics for `\"minus three minus two\"` (namely, `('-', ('~', 3), 2)`) is a tedious chore that most people probably could not perform reliably.  You need to understand Lisp-y prefix notation.  You need to remember to use the funny `'~'`, instead of the more natural `'-'`, for unary `\"minus\"`.  You need to remember to quote the operators.  And you need to get the order of operations right.  By contrast, writing down the target denotation (namely, `-5`) is easy as pie.  The only thing you really need to think about is the order of operations, which most people are capable of mastering.  So if you're asking only for denotations, rather than semantics, you can get more annotations, faster, cheaper, and more reliably, from ordinary people. -->\n",
    "\n",
    "\n",
    "<br>\n",
    "**One of the principal contributions of [Liang et al. 2011][] was to show that it is possible to learn scoring models for semantic parsing using only target denotations, rather than target semantics, as the source of supervision.  The central idea is presented with admirable clarity in [Liang & Potts 2015].**\n",
    "\n",
    "  [Liang et al. 2011]: http://www.cs.berkeley.edu/~jordan/papers/liang-jordan-klein-acl2011.pdf\n",
    "  [Liang & Potts 2015]: http://www.annualreviews.org/doi/pdf/10.1146/annurev-linguist-030514-125312\n",
    "\n",
    "In SippyCup, we can change the source of supervision from semantics to denotations simply by changing the training metric from `SemanticsAccuracyMetric` to `DenotationAccuracyMetric`.  With this change, a parse will count as correct iff the denotation of its semantic yield matches the target denotation in the example.\n",
    "\n",
    "Let's begin by repeating the experiment we did a moment ago, but switching to denotations as the source of supervision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ba197aa5fa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenotationAccuracyMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_test(model=arithmetic_model,\n\u001b[0m\u001b[1;32m      4\u001b[0m            \u001b[0mtrain_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marithmetic_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mtest_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marithmetic_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test' is not defined"
     ]
    }
   ],
   "source": [
    "from metrics import DenotationAccuracyMetric\n",
    "\n",
    "train_test(model=arithmetic_model,\n",
    "           train_examples=arithmetic_examples[:13],\n",
    "           test_examples=arithmetic_examples[13:],\n",
    "           training_metric=DenotationAccuracyMetric(),\n",
    "           seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're learning from denotations, the semantics accuracy on the training data did not reach 1.000, as it did before.  One of the [exercises](#arithmetic-exercises) will ask you to investigate why.  However, the denotation accuracy did reach 1.000.\n",
    "\n",
    "### <span style=\"color:red\"> Question: Why don't the test set results budge? </span>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file [`arithmetic.py`](./arithmetic.py) contains a set of 100 \"development\" examples for the arithmetic domain.  These examples are annotated only with denotations, not with semantics.  100 examples is still not huge, but the arithmetic domain is simple enough that it should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training examples, 4 test examples\n",
      "================================================================================\n",
      "Evaluating on 100 train examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 100 examples:\n",
      "\n",
      "denotation accuracy                0.640\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   4.520\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 4 test examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 4 examples:\n",
      "\n",
      "denotation accuracy                0.750\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.250\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Running SGD learning on 100 examples with training metric: denotation accuracy\n",
      "\n",
      "random.seed(1)\n",
      "SGD iteration 0: train accuracy: 0.670\n",
      "SGD iteration 1: train accuracy: 0.850\n",
      "SGD iteration 2: train accuracy: 0.890\n",
      "SGD iteration 3: train accuracy: 0.900\n",
      "SGD iteration 4: train accuracy: 0.900\n",
      "SGD iteration 5: train accuracy: 0.910\n",
      "SGD iteration 6: train accuracy: 0.910\n",
      "SGD iteration 7: train accuracy: 0.930\n",
      "SGD iteration 8: train accuracy: 0.930\n",
      "SGD iteration 9: train accuracy: 0.930\n",
      "\n",
      "Feature weights:\n",
      "     1.3\t('-', '+')\n",
      "     1.1\t('*', '~')\n",
      "     1.1\t('*', '+')\n",
      "     0.7\t('~', '-')\n",
      "     0.5\t('*', '-')\n",
      "     0.0\t('~', '+')\n",
      "    -0.5\t('+', '-')\n",
      "    -1.1\t('~', '*')\n",
      "    -1.1\t('-', '~')\n",
      "    -1.1\t('-', '*')\n",
      "    -1.1\t('+', '*')\n",
      "    -1.2\t('+', '~')\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 100 train examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 100 examples:\n",
      "\n",
      "denotation accuracy                0.930\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   4.520\n",
      "spurious ambiguity                 0.000\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 4 test examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 4 examples:\n",
      "\n",
      "denotation accuracy                1.000\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   1.250\n",
      "spurious ambiguity                 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from arithmetic import arithmetic_dev_examples\n",
    "from metrics import denotation_match_metrics\n",
    "\n",
    "train_test(model=arithmetic_model,\n",
    "           train_examples=arithmetic_dev_examples,\n",
    "           test_examples=arithmetic_examples[13:],\n",
    "           metrics=denotation_match_metrics(),\n",
    "           training_metric=DenotationAccuracyMetric(),\n",
    "           seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that denotation accuracy on the test examples now reaches 1.000 after training.  The sole error is corrected, because the larger training dataset affords ample opportunity to learn that `*` should take precedence over `+`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <span style=\"color:red\"> Exercises! </span> <a id=\"arithmetic-exercises\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: consider ordering. -->\n",
    "\n",
    "Note that some of these exercises ask you to extend the grammar in ways that will be a bit awkward, given the requirement that rules be in Chomsky normal form (CNF).   Your solutions to these exercises should adhere to the CNF restriction.  It's going to be a bit of a pain in the ass, and that's part of the point.\n",
    "\n",
    "### Straightforward\n",
    "\n",
    "1) How many parses do you get for \"one plus one plus one plus one plus one\"?  Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##\n",
    "# Hint: look at parse_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Extend the grammar to support postfix unary operators, as in \"two squared\" or \"two cubed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 13 rules.\n"
     ]
    }
   ],
   "source": [
    "operator_rules = [\n",
    "    Rule('$UnOp', 'minus', '~'),\n",
    "    Rule('$BinOp', 'plus', '+'),\n",
    "    Rule('$BinOp', 'minus', '-'),\n",
    "    Rule('$BinOp', 'times', '*'),\n",
    "    ## ADD CODE HERE ##\n",
    "]\n",
    "\n",
    "arithmetic_grammar = Grammar(arithmetic_rules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Extend the grammar to support divison, as in \"four divided by three\" or \"minus four over two\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 15 rules.\n"
     ]
    }
   ],
   "source": [
    "operator_rules = [\n",
    "    Rule('$UnOp', 'minus', '~'),\n",
    "    Rule('$BinOp', 'plus', '+'),\n",
    "    Rule('$BinOp', 'minus', '-'),\n",
    "    Rule('$BinOp', 'times', '*'),\n",
    "    ## ADD CODE HERE ##\n",
    "]\n",
    "\n",
    "arithmetic_grammar = Grammar(arithmetic_rules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Extend the grammar to support multi-word operators, as in \"the square root of one\" or \"the average of one and two\".  (You may need to extend the semantic representation and the executor as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Rule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48aefc710a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m operator_rules = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$UnOp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'~'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$BinOp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$BinOp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$BinOp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'times'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Rule' is not defined"
     ]
    }
   ],
   "source": [
    "operator_rules = [\n",
    "    Rule('$UnOp', 'minus', '~'),\n",
    "    Rule('$BinOp', 'plus', '+'),\n",
    "    Rule('$BinOp', 'minus', '-'),\n",
    "    Rule('$BinOp', 'times', '*'),\n",
    "    ## ADD CODE HERE ##\n",
    "]\n",
    "\n",
    "arithmetic_grammar = Grammar(arithmetic_rules())\n",
    "\n",
    "sems = [\n",
    "    ('+', 1, 1),                # one plus one\n",
    "    ('-', ('~', 3), 2),         # minus three minus two\n",
    "    ('-', ('+', 3, 3), 2),      # three plus three minus two\n",
    "    ('+', ('*', 2, 2), 3),      # two times two plus three\n",
    "    ## ADD CODE HERE ##\n",
    "]\n",
    "\n",
    "ops = {\n",
    "    '~': lambda x: -x,\n",
    "    '+': lambda x, y: x + y,\n",
    "    '-': lambda x, y: x - y,\n",
    "    '*': lambda x, y: x * y,\n",
    "    ## ADD CODE HERE ##\n",
    "}\n",
    "\n",
    "\n",
    "for sem in sems:\n",
    "    print('{} = {}'.format(sem, execute(sem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) When we learned from semantics, semantics accuracy on the training examples reached 1.000 after training.  However, when we switched to learning from denotations, it did not.  Explain why.  Be precise and specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight Forward (do this if you have time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) When we trained on the 100 examples in `arithmetic_dev_examples`, denotation accuracy on the training examples did not reach 1.000 after training.  Diagnose the errors and describe your findings.  What could help us to eliminate those errors?  (You don't need to implement the fix.  Just give a precise diagnosis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2015 Bill MacCartney"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
