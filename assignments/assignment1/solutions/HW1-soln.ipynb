{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1, DS-GA 1012, Spring 2019\n",
    "\n",
    "## Due Feburary 13, 2019 at 2pm (ET)\n",
    "\n",
    "Download the data zip `DS-GA1012-hw1-data.zip`. Complete the following questions in the notebook and submit your completed notebook on NYU Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring effect of context size [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face many implicit and explicit design decisions in creating distributional word representations. For example, in lecture and in lab, we created word vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Write `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. Feel free to modify the code used in lab [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(data, max_vocab_size=20000, context_size=1):\n",
    "    \"\"\" Build a co-occurrence matrix\n",
    "    \n",
    "    args:\n",
    "        - data: iterable where each item is a list of tokens (string) \n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        - context_size: window around a word that is considered context\n",
    "            context_size=1 should consider pairs of adjacent words\n",
    "            \n",
    "    returns:\n",
    "        - co-occurrence matrix: numpy array where row i corresponds to the co-occurrence counts for word i\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of `build_cooccurrence_matrix` to generate the co-occurence matrix from the sentences of [SST](http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip) (file `datasetSentences.txt`) with `context_size=2` and `max_vocab_size=10000`. What is the co-occurrence count of the words \"the\" and \"end\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def build_cooccurrence_matrix(data, max_vocab_size=20000, context_size=2, verbose=True):\n",
    "    \"\"\" \n",
    "    \n",
    "    args:\n",
    "        - data: iterable where each item is a string sentence\n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        \n",
    "    returns:\n",
    "        - coocur_mat: co-occurrence matrix as a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_token_frequencies(context_size):\n",
    "        tok2freq = defaultdict(int)\n",
    "        coocur_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for datum in data:\n",
    "            for i, tok in enumerate(datum):\n",
    "                tok2freq[tok] += 1\n",
    "                for k in range(context_size + 1):\n",
    "                    if i + k < len(datum):\n",
    "                        coocur_counts[tok][datum[i+k]] += 1\n",
    "                        if k != 0:\n",
    "                            coocur_counts[datum[i+k]][tok] += 1\n",
    "        return tok2freq, coocur_counts\n",
    "    \n",
    "    def prune_vocabulary(tok2freq, max_vocab_size):\n",
    "        \"\"\" Prune vocab by taking max_vocab_size most frequent words \"\"\"\n",
    "        tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
    "        tok_and_freqs.sort(key = lambda x: x[1], reverse=True) # sorts in-place\n",
    "        tok2idx = {tok: idx for idx, (tok, _) in enumerate(tok_and_freqs[:max_vocab_size])}\n",
    "        idx2tok = {idx: tok for tok, idx in tok2idx.items()}\n",
    "        return tok2idx, idx2tok\n",
    "    \n",
    "    def _build_coocurrence_mat(idx2tok, coocur_counts):\n",
    "        #mat = [[coocur_counts[idx2tok[i]][idx2tok[j]] for j in range(len(idx2tok))] for i in range(len(idx2tok))]\n",
    "        vocab_size = len(idx2tok)\n",
    "        mat = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(i, vocab_size):\n",
    "                if coocur_counts[idx2tok[i]][idx2tok[j]]:\n",
    "                    mat[i][j] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "                    mat[j][i] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "        return np.array(mat)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Counting words...\")\n",
    "    start_time = time.time()\n",
    "    tok2freq, coocur_counts = get_token_frequencies(context_size)\n",
    "    if verbose:\n",
    "        print(\"\\tFinished counting %d words in %.5f\" % (len(tok2freq), time.time() - start_time))\n",
    "        print(\"Pruning vocabulary...\")\n",
    "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
    "    start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"\\tFinished pruning vocabulary to %d words in %.5f\" % (len(tok2idx), time.time() - start_time))\n",
    "        print(\"Building co-occurrence matrix...\")\n",
    "    start_time = time.time()\n",
    "    coocur_mat = _build_coocurrence_mat(idx2tok, coocur_counts)\n",
    "    if verbose:\n",
    "        print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
    "    return coocur_mat, tok2idx, idx2tok, coocur_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting 21701 words in 0.62653\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary to 10000 words in 0.00001\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 38.50821\n"
     ]
    }
   ],
   "source": [
    "def load_sst(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        data_fh.readline() # skip the header\n",
    "        raw_data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
    "    data = [d.strip().split() for d in raw_data]\n",
    "    return data\n",
    "\n",
    "data = load_sst('hw1/DS-GA1012-hw1/datasetSentences.txt')\n",
    "mat, tok2idx, idx2tok, _ = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the and end co-occur 98 times\n"
     ]
    }
   ],
   "source": [
    "tok1 = \"the\"\n",
    "tok2 = \"end\"\n",
    "print(\"%s and %s co-occur %d times\" % (tok1, tok2, mat[tok2idx[tok1]][tok2idx[tok2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Plot the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (see lab). [12 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def vector_length(u):\n",
    "    \"\"\"Length (L2) of the 1d np.array `u`. Returns a new np.array with the \n",
    "    same dimensions as `u`.\"\"\"\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def cosine_similarity(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    return (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
    "\n",
    "def load_word_similarity_dataset(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        raw_data = data_fh.readlines()\n",
    "    data = []\n",
    "    trgs = []\n",
    "    for datum in raw_data:\n",
    "        datum = datum.strip().split(',')\n",
    "        data.append((datum[0], datum[1]))\n",
    "        trgs.append(float(datum[2]))\n",
    "    return data, trgs\n",
    "\n",
    "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
    "    \"\"\" \"\"\"\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    n_exs = 0\n",
    "    for (word1, word2), trg in zip(word_pairs, targets):\n",
    "        if word1 in tok2idx and word2 in tok2idx:\n",
    "            pred_sim = cosine_similarity(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
    "            preds.append(pred_sim)\n",
    "            trgs.append(trg)\n",
    "            n_exs += 1\n",
    "    \n",
    "    rho, pvalue = spearmanr(trgs, preds)\n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting 21701 words in 0.60564\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary to 10000 words in 0.00001\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 34.02603\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting 21701 words in 0.92447\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary to 10000 words in 0.00002\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 41.55725\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting 21701 words in 0.99132\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary to 10000 words in 0.00001\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 35.06092\n",
      "Evaluated on 248 of 771 examples\n",
      "Counting words...\n",
      "\tFinished counting 21701 words in 1.02303\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary to 10000 words in 0.00001\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 38.45037\n",
      "Evaluated on 248 of 771 examples\n",
      "[-0.015468056503250562, 0.06267503435290189, 0.09901335966261092, 0.11281183122379504]\n"
     ]
    }
   ],
   "source": [
    "test_file = 'hw1/DS-GA1012-hw1/MTURK-771.csv'\n",
    "test_data, test_trgs = load_word_similarity_dataset(test_file)\n",
    "scores = []\n",
    "context_sizes = [1, 2, 3, 4]\n",
    "for context_size in context_sizes:\n",
    "    mat, tok2idx, idx2tok, _ = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=context_size)\n",
    "    score = evaluate_word_similarity(test_data, test_trgs, mat, tok2idx)\n",
    "    scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Word similarity correlation')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(context_sizes, scores, marker='o')\n",
    "plt.xlabel(\"Context size\")\n",
    "plt.ylabel(\"Word similarity correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Briefly discuss the pros and cons of varying (i) the context size (ii) the vocabulary size (iii) using bigrams instead of unigrams (iv) using subword tokens instead of words. [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: (i) Larger contexts are generally better for capturing semantics of a word (by capturing longer range correlations), smaller contexts are generally better for capturing syntax of a word.\n",
    "\n",
    "(ii) With a larger vocab size, we have fewer OOV words, but we must keep an additional vector for each word (and possible an extra dimension for each other word vector, depending on representation). Any losses in speed depend on how your using these word vectors. If you're using them as a lookup table, then we might see small to no loss; if we're performing operations over the entire matrix, then we'll likely see larger slow downs due to increasing vocab size.\n",
    "\n",
    "(iii) Some concepts might be better represented as bigrams or other higher $n$-grams, but our co-occurrence matrix would likely become very sparse and balloon in size (naively, but this isn't really an issue if you're only taking the top $k$ most frequent tokens). Practically speaking, it isn't that much slower to preprocess the text into bigrams, but there are more of them, so we might see a slow down in building our vocabulary (e.g. if we're sorting bigrams by frequency), but this depends on implementation.\n",
    "\n",
    "(iv) We can share information between word parts, e.g. lemmas or suffixes, but we need to specify how to break up words into useful subparts and how to combine subparts to form overall token representations. Note that subword tokens does not necessarily mean we're splitting words into characters or prefix/lemma/affix; several state of the art NLU models use BPE. We also artificially increase the length of the sequence by using subparts, which might make it harder to learn with some optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pointwise Mutual Information [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we introduced __pointwise mutual information__ (PMI), which addresses the issue of normalization removing information about absolute magnitudes of counts. The PMI for word $\\times$ context pair $(w,c)$ is \n",
    "\n",
    "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)$$\n",
    "\n",
    "with $\\log(0) = 0$. This is a measure of how far that cell's value deviates from what we would expect given the row and column sums for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement `pmi`, a function which takes in a co-occurence matrix and returns the matrix with PMI normalization applied. [15 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat):\n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PMI to the co-occurence matrix computed above with `context_size=1`. What is the PMI between the words \"the\" and \"end\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat, ppmi=True):\n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI\n",
    "        - ppmi: boolean; compute PPMI if True\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def _pmi_log_soln(x, positive=True):\n",
    "        \"\"\"With `positive=False`, return log(x) if possible, else 0.\n",
    "        With `positive=True`, log(x) is mapped to 0 where negative.\"\"\"\n",
    "        val = 0.0\n",
    "        if x > 0.0:\n",
    "            val = np.log(x)\n",
    "        if positive:\n",
    "            val = max([val,0.0])\n",
    "        return val\n",
    "    \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log_soln(x, positive=ppmi)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmat = pmi(mat, ppmi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-end PMI: 1.399\n"
     ]
    }
   ],
   "source": [
    "tok1 = \"the\"\n",
    "tok2 = \"end\"\n",
    "print(\"%s-%s PMI: %.3f\" % (tok1, tok2, pmat[tok2idx[tok1]][tok2idx[tok2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. We also consider an extension of PMI, positive PMI (PPMI), that maps all negative PMI values to 0.0 ([Levy and Goldberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)). \n",
    "Write `ppmi`, which is the same as `pmi` except it applies PPMI instead of PMI (feel free to implement it as an option of `pmi`). What is the PMI of the words \"the\" and \"start\"? The PPMI? [5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmat = pmi(mat, ppmi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-start PMI: -0.035\n",
      "the-start PPMI: 0.000\n"
     ]
    }
   ],
   "source": [
    "tok1 = \"the\"\n",
    "tok2 = \"start\"\n",
    "print(\"%s-%s PMI: %.3f\" % (tok1, tok2, pmat[tok2idx[tok1]][tok2idx[tok2]]))\n",
    "print(\"%s-%s PPMI: %.3f\" % (tok1, tok2, ppmat[tok2idx[tok1]][tok2idx[tok2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing PMI [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Consider the matrix `np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])`. Reweight this matrix using `ppmi`. (i) What is the value obtained for cell `[0,0]`, and (ii) give a brief description for what is likely problematic about this value. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: 1.609 is far too large of a value relative to the rest of the matrix. PMI is biased towards infrequent events and so it gives this far too large of a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.60893804 0.         0.        ]\n",
      " [0.         0.         0.28788209]\n",
      " [0.22289371 0.51107566 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tmp = pmi(np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]]))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Give a suggestion for dealing with the problematic value and explain why it deals with this. Demonstrate your suggestion empirically [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__: Use add-2 smoothing or any other kind of smoothing. This creates pseudocounts of rare events making them relatively less rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76044454, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.28798133],\n",
       "       [0.22219683, 0.51037628, 0.        ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pmi(np.array([[3.0, 2.0, 2.0], [1002.0, 1002.0, 4002.0], [1002.0, 2002.0, 1001.0]]), ppmi=True)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Consider starting with a word-word co-occurence matrix and applied PMI to this matrix. (i) Which of the following describe the resulting vectors: sparse, dense, high-dimensional, low-dimensional (ii) If you wanted the opposite style of representation, what could you do? [5 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__:\n",
    "\n",
    "i. sparse, high-dimensional\n",
    "\n",
    "ii. LSA or SVD-inspired methods as they are dense and relatively low-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Analogy Evaluation [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word analogies provide another kind of evaluation for distributed representations. Here, we are given three vectors A, B, and C, in the relationship\n",
    "\n",
    "_A is to B as C is to __ _\n",
    "\n",
    "and asked to identify the fourth that completes the analogy. These analogies are by and large substantially easier than the classic brain-teaser analogies that used to appear on tests like the SAT, but it's still an interesting, demanding\n",
    "task. \n",
    "\n",
    "The core idea is that we make predictions by creating the vector $(A - B) + C$,ranking all vectors based on their distance from this new vector, and choosing the closest as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement the function `analogy_completion`. [9 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_completion(a, b, c, mat):\n",
    "    \"\"\" \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_completion(a, b, c, mat, tok2idx, idx2tok):\n",
    "    \"\"\"Compute ? in \n",
    "    a is to b as c is to ? \n",
    "    as the closest to (b-a) + c\n",
    "    \n",
    "    args:\n",
    "        - a, b, c: words (strings)\n",
    "        - mat: matrix of word vectors\n",
    "        - idx2tok (optional, depending on your implementation): dict mapping matrix rows to tokens\n",
    "        - tok2idx (optional, depending on your implementation): dict mapping tokens to matrix rows\n",
    "    \n",
    "    returns:\n",
    "        - list of the top words\n",
    "    \"\"\"\n",
    "    \n",
    "    def cosine_dist(u, v):        \n",
    "        \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "        the same dimensionality. Returns a float.\"\"\"\n",
    "        return 1.0 - (np.dot(u, v) / (np.sqrt(np.dot(u, u)) * np.sqrt(np.dot(v, v))))\n",
    "\n",
    "    for x in (a, b, c):\n",
    "        if x not in tok2idx:\n",
    "            raise ValueError('%s is OOV!' % x)\n",
    "    avec = mat[tok2idx[a]]\n",
    "    bvec = mat[tok2idx[b]]\n",
    "    cvec = mat[tok2idx[c]]\n",
    "    newvec = (bvec - avec) + cvec\n",
    "    dists = [(w, cosine_dist(newvec, mat[i])) for w, i in tok2idx.items() if w not in (a, b, c)]\n",
    "    return [w for w, _ in sorted(dists, key=lambda x: x[1], reverse=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Our simple word embeddings likely won't perform well on this task. Let's instead look at some high quality pretrained word embeddings. Write code to load 300-dimensional [GloVe word embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) trained on 840B tokens. Each line of the file is formatted as a word followed by 300 floats that make up its corresponding word embedding (all space delimited). The entries of GloVe word embeddings are not counts, but instead are learned via machine learning. Use your `analogy_completion` code to complete the following analogies using the GloVe word embeddings. [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Beijing\" is to \"China\" as \"Paris\" is to ?\n",
    "- \"gold\" is to \"first\" as \"silver\" is to ?\n",
    "- \"Italian\" is to \"mozzarella\" as \"American\" is to ?\n",
    "- \"research\" is to \"fun\" as \"engineering\" is to ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file, n_vecs=20000):\n",
    "    \"\"\" \"\"\"\n",
    "    tok2vec = {}\n",
    "    with open(glove_file, 'r') as glove_fh:\n",
    "        for i, row in enumerate(glove_fh):\n",
    "            word, vec = row.split(' ', 1)\n",
    "            tok2vec[word] = np.array([float(n) for n in vec.split(' ')])\n",
    "            if i >= n_vecs:\n",
    "                break\n",
    "    return tok2vec\n",
    "\n",
    "def glove_analogy_completion(a, b, c, vecs):\n",
    "    \"\"\"Compute ? in \n",
    "    a is to b as c is to ? \n",
    "    as the closest to (b-a) + c\n",
    "    \n",
    "    args:\n",
    "        - a, b, c: words (strings)\n",
    "        - mat: matrix of word vectors\n",
    "        - idx2tok (optional, depending on your implementation): dict mapping matrix rows to tokens\n",
    "        - tok2idx (optional, depending on your implementation): dict mapping tokens to matrix rows\n",
    "    \n",
    "    returns:\n",
    "        - list of the top words\n",
    "    \"\"\"\n",
    "    \n",
    "    def cosine_dist(u, v):        \n",
    "        \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "        the same dimensionality. Returns a float.\"\"\"\n",
    "        return 1.0 - (np.dot(u, v) / (np.sqrt(np.dot(u, u)) * np.sqrt(np.dot(v, v))))\n",
    "\n",
    "    for x in (a, b, c):\n",
    "        if x not in vecs:\n",
    "            raise ValueError('%s is OOV!' % x)\n",
    "    avec = vecs[a]\n",
    "    bvec = vecs[b]\n",
    "    cvec = vecs[c]\n",
    "    newvec = (bvec - avec) + cvec\n",
    "    dists = [(w, cosine_dist(newvec, vec)) for w, vec in vecs.items() if w not in (a, b, c)]\n",
    "    return sorted(dists, key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d5c79984cec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.840B.300d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglove_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4f34b3d8cb08>\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(glove_file, n_vecs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtok2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mglove_fh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_fh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "glove_file = \"glove.840B.300d.txt\"\n",
    "glove_vecs = load_glove(glove_file, n_vecs=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheddar'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_analogy_completion(\"Italian\", \"mozzarella\", \"American\", glove_vecs)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Let's get a more quantitative, aggregate sense of the quality of GloVe embeddings. Load the analogies from `gram6-nationality-adjective.txt` and evaluate GloVe embeddings. Report the mean reciprocal rank of the correct answer (the last word on each line) for each analogy. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_evaluation(glove_vecs, test_file, verbose=False):\n",
    "    \"\"\"Basic analogies evaluation for a file `src_filename`\n",
    "    in `question-data/`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    mat : 2d np.array\n",
    "        The VSM being evaluated.\n",
    "        \n",
    "    rownames : list of str\n",
    "        The names of the rows in `mat`.\n",
    "        \n",
    "    src_filename : str\n",
    "        Basename of the file to be evaluated. It's assumed to be in\n",
    "        `vsmdata_home`/question-data.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (float, float)\n",
    "        The first is the mean reciprocal rank of the predictions and \n",
    "        the second is the accuracy of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read in the data and restrict to problems we can solve:\n",
    "    data = [line.split() for line in open(test_file).read().splitlines()]\n",
    "    data = [prob for prob in data if set(prob) <= set(glove_vecs.keys())]\n",
    "    # Run the evaluation, collecting accuracy and rankings:\n",
    "    results = defaultdict(int)\n",
    "    ranks = []\n",
    "    for a, b, c, d in data:\n",
    "        predicted = glove_analogy_completion(a, b, c, glove_vecs)\n",
    "        if verbose:\n",
    "            print(\"%s is to %s as %s is to %s (actual is %s)\" % (a, b, c, predicted[0][0], d))\n",
    "        results[predicted[0][0] == d] += 1\n",
    "        predicted_words, _ = zip(*predicted)\n",
    "        ranks.append(predicted_words.index(d))\n",
    "    # Return the mean reciprocal rank and the accuracy results:\n",
    "    mrr = np.mean(1.0/(np.array(ranks)+1))\n",
    "    return (mrr, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9391509433962264, defaultdict(int, {True: 97, False: 9}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_evaluation(glove_vecs, \"gram6-nationality-adjective.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
