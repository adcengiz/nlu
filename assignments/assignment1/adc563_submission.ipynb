{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Natural Language Understanding DS-GA 1012 Homework 1</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Due Feburary 13, 2019 at 2pm (ET)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Exploring effect of context size [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face many implicit and explicit design decisions in creating distributional word representations. For example, in lecture and in lab, we created word vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurence Matrix\n",
    "__a__. Write `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. Feel free to modify the code used in lab [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_(string, list_of_char):\n",
    "    for x in list_of_char:\n",
    "        string = string.replace(str(x), \"\")\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(filepath, mode=\"w\"):\n",
    "    \"\"\"args\n",
    "        - filepath: path to text file\n",
    "        - mode: \"w\" for word, \"s\" for sentence list\n",
    "    \n",
    "    returns:\n",
    "        - text_list: word or sentence list depending on the mode\"\"\"\n",
    "    \n",
    "    text = open(filepath, \"r\")\n",
    "    \n",
    "    if mode == \"w\":\n",
    "        text_list = text.read().replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "        text_list = remove_(text_list,range(10)).lower().split(\" \")[1:]\n",
    "    elif mode == \"s\":\n",
    "        text_list = text.read().split(\"\\n\")\n",
    "        text_list = [sent.replace(\"\\t\",\"\") for sent in text_list]\n",
    "        text_list = [remove_(x,range(10)).lower().split(\" \") for x in text_list][1:]\n",
    "\n",
    "    else:\n",
    "        raise ValueError (\"mode must be 'w'(word) or 's'(sentence)!\")\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentence = text_to_list(\"data/datasetSentences.txt\", \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word = text_to_list(\"data/datasetSentences.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(data, \n",
    "                              max_vocab_size=10000, \n",
    "                              context_size=1):\n",
    "    \n",
    "    \"\"\" Build a co-occurrence matrix\n",
    "    \n",
    "    args:\n",
    "        - data: iterable where each item is a list of tokens (string) \n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        - context_size: window around a word that is considered context\n",
    "            context_size=1 should consider pairs of adjacent words\n",
    "            \n",
    "    returns:\n",
    "        - co-occurrence matrix: numpy array where row i corresponds \n",
    "        to the co-occurrence counts for word i\"\"\"\n",
    "    \n",
    "    assert (type(data) == list or type(data) == np.ndarray), \"First input must be a list or a numpy ndarray!\"\n",
    "    \n",
    "    if type(data) == list:\n",
    "        assert (len(data) > 0), \"Data must be non-empty.\"\n",
    "    else:\n",
    "        assert (data.shape[0] > 0), \"Data must be non-empty.\"\n",
    "        \n",
    "    ## assuming data is a list of sentences (each split into tokens)\n",
    "    word_data = ((\" \").join([(\" \").join(x) for x in data])).split(\" \")\n",
    "    word2count = Counter(word_data)\n",
    "    sorted_by_freq = sorted(word2count.items(), \n",
    "                            key=lambda kv: kv[1])\n",
    "    \n",
    "    # vocab = {word: count} for the most frequent max_vocab_size words\n",
    "    vocab = dict(sorted_by_freq[-max_vocab_size:])\n",
    "    keys = [*vocab.keys()]\n",
    "    \n",
    "    token2id = {}\n",
    "    id2token = {}\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = [x if x in keys else \"<UNK>\" for x in data[i]]\n",
    "        \n",
    "    f_keys = keys + [\"<UNK>\"]\n",
    "    \n",
    "#     print (f_keys)\n",
    "    \n",
    "    for token in f_keys:\n",
    "        token2id[token] = f_keys.index(token)\n",
    "        \n",
    "#     print (token2id)\n",
    "        \n",
    "    for j in range(len(f_keys)):\n",
    "        id2token[j] = f_keys[j]\n",
    "        \n",
    "#     print (id2token)\n",
    "\n",
    "    edge = len(f_keys) - 1\n",
    "    comatrix = np.zeros((edge, edge))\n",
    "\n",
    "    # dict where each key is a unique token and each value is another dict of other unique keys \n",
    "    # that hold cooccurrence counts\n",
    "    occurrences = OrderedDict((key, OrderedDict((key, 0) for key in f_keys)) for key in f_keys)\n",
    "\n",
    "    for sent in data:\n",
    "        sent_length = len(sent)\n",
    "        if context_size >= sent_length:\n",
    "            for i in range(sent_length):\n",
    "                for item in sent:\n",
    "                    occurrences[sent[i]][item] += 1\n",
    "        else:\n",
    "            for i in range(sent_length):\n",
    "                if i <= context_size and sent_length - context_size - 1 <= i:\n",
    "                    for item in sent:\n",
    "                        occurrences[sent[i]][item] += 1\n",
    "                elif i <= context_size and sent_length - context_size - 1 > i:\n",
    "                    for item in sent[:i+context_size+1]:\n",
    "                        occurrences[sent[i]][item] += 1\n",
    "                elif i > context_size and sent_length - context_size - 1 > i:\n",
    "                    for item in sent[i-context_size:i+context_size+1]:\n",
    "                        occurrences[sent[i]][item] += 1\n",
    "                elif i > context_size and sent_length - context_size - 1 <= i:\n",
    "                    for item in sent[i-context_size:]:\n",
    "                        occurrences[sent[i]][item] += 1\n",
    "\n",
    "    for i in range(edge):\n",
    "        for key in f_keys:\n",
    "            if token2id[key] == i:\n",
    "                comatrix[token2id[key]] = np.array([occurrences[key][co] for co in [*occurrences[key].keys()]])\n",
    "                    \n",
    "    return token2id, id2token, comatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[\"this\", \"is\", \"a\", \"sentence\"], [\"and\", \"this\", \"is\", \"another\",\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'a': 0,\n",
       "  'and': 1,\n",
       "  'another': 2,\n",
       "  'this': 3,\n",
       "  'is': 4,\n",
       "  'sentence': 5,\n",
       "  '<UNK>': 6},\n",
       " {0: 'a',\n",
       "  1: 'and',\n",
       "  2: 'another',\n",
       "  3: 'this',\n",
       "  4: 'is',\n",
       "  5: 'sentence',\n",
       "  6: '<UNK>'},\n",
       " array([[1., 0., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 2., 2., 2., 0.],\n",
       "        [1., 1., 1., 2., 2., 2., 0.],\n",
       "        [1., 0., 1., 2., 2., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_cooccurrence_matrix(A, context_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token2id, id2token, co_matrix = build_cooccurrence_matrix(data_sentence, max_vocab_size=10000, context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix for Sentence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of `build_cooccurrence_matrix` to generate the co-occurence matrix from the sentences of [SST](http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip) (file `datasetSentences.txt`) with `context_size=2` and `max_vocab_size=10000`. What is the co-occurrence count of the words \"the\" and \"end\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token, co_matrix = build_cooccurrence_matrix(data_sentence, \n",
    "                                                          max_vocab_size=10000, \n",
    "                                                          context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the id: 9998\n",
      "end id: 9803\n"
     ]
    }
   ],
   "source": [
    "print (f'the id: {token2id[\"the\"]}')\n",
    "print (f'end id: {token2id[\"end\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_matrix[token2id[\"the\"]][token2id[\"end\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: The cooccurrence count of the words 'the' and 'end' is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Size Effect\n",
    "__b__. Plot the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (see lab). [12 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "To obtain the Spearman correlation between Mturk similarity data and cooccurrence matrix:\n",
    "\n",
    "Each token couple's similarity was determined by evaluating the cosine similarity of their corresponding rows on the cooccurrence matrix. For instance to calculate the similarity of \"word\" and \"token\":\n",
    "\n",
    "    * we look up the ids of \"word\" and \"token\" from the token2id dictionary, \n",
    "    * slice their rows from the cooccurrence matrix,\n",
    "    * measure the cosine similarity of those two vectors.\n",
    "    \n",
    "After this, we end up with a single scalar defining similarity between \"word\" and \"token\". We repeat the same thing for all the word couples present in the mturk dataset, and compute the correlation of two nx1 vectors.\n",
    "\n",
    "__Please see the plot below in this section.__\n",
    "\n",
    "__Cosine Similarity__ for vectors u and v is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\left(\\frac{\\sum_{i=1}^{n} u_{i} \\cdot v_{i}}{\\|u\\|\\cdot \\|v\\|}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    co_sim = np.dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return co_sim\n",
    "\n",
    "def get_similarity(token1, token2, mturk_df, \n",
    "                   token2id, cooccurrence_matrix):\n",
    "    \n",
    "    mturk_similarity = float(mturk_df[(mturk_df[\"word1\"]==token1)\\\n",
    "                                      &(mturk_df[\"word2\"]==token2)][\"similarity\"])\n",
    "    \n",
    "    keys = [*token2id.keys()]\n",
    "    \n",
    "    if token1 not in keys:\n",
    "        covector_1 = np.zeros((cooccurrence_matrix.shape[0],))\n",
    "    else:\n",
    "        covector_1 = cooccurrence_matrix[token2id[token1]]\n",
    "    \n",
    "    if token2 not in keys:\n",
    "        covector_2 = np.zeros((cooccurrence_matrix.shape[0],))\n",
    "    else:\n",
    "        covector_2 = cooccurrence_matrix[token2id[token2]]\n",
    "\n",
    "    co_similarity = cosine_similarity(covector_1, covector_2)\n",
    "    \n",
    "    return mturk_similarity, co_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_data = pd.DataFrame(pd.read_csv(\"data/MTURK-771.csv\",header=None))\n",
    "mturk_data.columns = [\"word1\", \"word2\", \"similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_vector(similarity_df, \n",
    "                          token2id,\n",
    "                          comatrix):\n",
    "    \"\"\"Takes as input:\n",
    "    - Similarity df: mturk similarity dataset with columns:['word1','word2','similarity'],\n",
    "    - token2id: token2id vocabulary generated for sentence dataset,\n",
    "    - comatrix: cooccurrence matrix generated from sentence dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - mturk similarity vector\n",
    "    - cosine similarity vector generated from the comatrix (cooccurrence matrix)\"\"\"\n",
    "    \n",
    "    num_ = similarity_df.shape[0]\n",
    "    sim_v = np.zeros((num_, 2))\n",
    "    \n",
    "    for i in range(num_):\n",
    "        sim_v[i,:] = get_similarity(similarity_df.iloc[i][\"word1\"], similarity_df.iloc[i][\"word2\"],\n",
    "                                    similarity_df, token2id, comatrix)\n",
    "        \n",
    "    sim_v = sim_v[np.where(np.isnan(sim_v)[:,1]==False)]\n",
    "        \n",
    "    return sim_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate_spearman(v1, v2):\n",
    "    \"\"\"Takes as input:\n",
    "    - v1: vector of size nx1 (mturk similairty vector)\n",
    "    - v2: vector of size nx1 (sentence dataset similarity vector)\n",
    "    \n",
    "    Returns:\n",
    "    - Spearman Correlation between v1 and v2.\n",
    "    \"\"\"\n",
    "    s = spearmanr(v1, v2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context Size = 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.0450713061232414, pvalue=0.43979008973141476)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_c1, id2token_c1, co_matrix_c1 = build_cooccurrence_matrix(data_sentence, \n",
    "                                                                   max_vocab_size=10000, \n",
    "                                                                   context_size=1)\n",
    "similarity_c1 = get_similarity_vector(mturk_data, token2id_c1, co_matrix_c1)\n",
    "spearman_c1 = evaluate_spearman(similarity_c1[:,0], similarity_c1[:,1])\n",
    "\n",
    "spearman_c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context Size = 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.08582998433844517, pvalue=0.14071154144449793)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_c2, id2token_c2, co_matrix_c2 = token2id, id2token, co_matrix\n",
    "similarity_c2 = get_similarity_vector(mturk_data, token2id_c2, co_matrix_c2)\n",
    "spearman_c2 = evaluate_spearman(similarity_c2[:,0], similarity_c2[:,1])\n",
    "\n",
    "spearman_c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context Size = 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.08563814153705145, pvalue=0.14160465475500314)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_c3, id2token_c3, co_matrix_c3 = build_cooccurrence_matrix(data_sentence, \n",
    "                                                                   max_vocab_size=10000, \n",
    "                                                                   context_size=3)\n",
    "similarity_c3 = get_similarity_vector(mturk_data, token2id_c3, co_matrix_c3)\n",
    "spearman_c3 = evaluate_spearman(similarity_c3[:,0], similarity_c3[:,1])\n",
    "spearman_c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context Size = 4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.1040173280300731, pvalue=0.07396016951421722)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id_c4, id2token_c4, co_matrix_c4 = build_cooccurrence_matrix(data_sentence, \n",
    "                                                                   max_vocab_size=10000,\n",
    "                                                                   context_size=4)\n",
    "similarity_c4 = get_similarity_vector(mturk_data, token2id_c4, co_matrix_c4)\n",
    "spearman_c4 = evaluate_spearman(similarity_c4[:,0], similarity_c4[:,1])\n",
    "spearman_c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e564b1908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = [spearman_c1, spearman_c2, spearman_c3, spearman_c4]\n",
    "c = range(1,5)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(c, [x[0] for x in s], linewidth=3.5, alpha=0.6,\n",
    "         label=\"Spearman Corr\", marker=\"o\", markersize=10)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.xlabel(\"Context Size\")\n",
    "plt.ylabel(\"Spearman Correlation\")\n",
    "plt.title(\"Correlation for Varying Context Size\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Briefly discuss the pros and cons of varying \n",
    "\n",
    "    I.  the context size \n",
    "    II.  the vocabulary size \n",
    "    III. using bigrams instead of unigrams \n",
    "    IV. using subword tokens instead of words. [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I. Context Size__\n",
    "\n",
    "As it can be seen on the above plot, increasing context size while constructing the cooccurrence matrix from the sentence data results in higher Spearman correlation between the MTurk similarity data and the similarity vector we have obtained. Below is a discussion of the pros and cons of increasing context size. \n",
    "\n",
    "__Pros__:\n",
    "  - It allows us to obtain a more __semantic__, rather than __syntactic__, representation of tokens. For example \"and\" and \"the\" occur in the same context very frequently, and this informs us about the syntactic structure of the text. However one occurring in the context does not provide us with information about the other token's meaning. A more semantic representation can be obtained by increasing the context size. \n",
    "  - For a given vocabulary size, increasing context size will result in a more dense and intrinsically informative co-occurrence matrix (that does not require much smoothing etc.). \n",
    "  \n",
    "__Cons__:\n",
    "  - Too large context size may result in a skewed distribution of co-occurrences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__II. Vocabulary Size__\n",
    "\n",
    "__Pros__:\n",
    "- Increasing vocabulary size will result in a __higher-dimensional representation__ for each token, which should lead to word embeddings that can capture more information. \n",
    "\n",
    "__Cons__: \n",
    "- As increasing the vocabulary size results in higher-dimensional representations, it will automatically increase the __memory cost__ of the application. \n",
    "- Once we increase the vocabulary size, less frequent tokens will be a part of the vocabulart and there will be a higher probability of some tokens not occuring at all in each other's context. This will lead to __sparse co-occurrence matrices__ for a given context size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__III. Using Bigrams__\n",
    "\n",
    "The effect of varying the n-gram setting depends highly on the __application area__. Therefore, I tried to gather more area-independent pros and cons that I had come up with. \n",
    "\n",
    "__Pros__:\n",
    "- Since bigrams are likely to be semantically more informative than unigrams, using bigrams should result in better vector representations. \n",
    "- Using bigrams might help the model deal with tokens that have __multiple meanings__. In other words, it can make easier for the model to differentiate among multiple meanings. \n",
    "\n",
    "__Cons__: \n",
    "- The co-occurrence chance of bigrams will be lower than unigrams, therefore this will result in a more __sparse__ co-occurrence matrix. \n",
    "- If we are not usings unigrams at all, and using only bigrams, this would make dealing with synonyms harder since the individual representations of the synonyms are no longer available and the model might not be able to assess whether two words are interchangeable or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IV. Using Subword Tokens__\n",
    "\n",
    "__Pros__:\n",
    "- Using subword tokens, instead of inflected forms or wordforms, will result in __less sparse__ representations, since different forms of the same word will appear as the same token in the vocabulary. \n",
    "\n",
    "__Cons__:\n",
    "- There might be some words that appear with specific wordforms and therefore replacing those wordforms with subword tokens could result in information loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pointwise Mutual Information [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we introduced __pointwise mutual information__ (PMI), which addresses the issue of normalization removing information about absolute magnitudes of counts. The PMI for word $\\times$ context pair $(w,c)$ is \n",
    "\n",
    "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)$$\n",
    "\n",
    "with $\\log(0) = 0$. This is a measure of how far that cell's value deviates from what we would expect given the row and column sums for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI\n",
    "\n",
    "__a__. Implement `pmi`, a function which takes in a co-occurence matrix and returns the matrix with PMI normalization applied. [15 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat):\n",
    "    \n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI (matrix that holds raw cooccurrence counts)\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"    \n",
    "    # p(w,c)\n",
    "    mat_p = np.divide(mat, np.sum(mat))\n",
    "    \n",
    "    # p(w), p(c)\n",
    "    p_w, p_c = np.sum(mat_p, 1), np.sum(mat_p, 0)\n",
    "\n",
    "    # construct p(w)*p(c)\n",
    "    d = np.outer(p_c, p_w)\n",
    "    \n",
    "    mat_p = np.divide(mat_p, d.T)\n",
    "    \n",
    "    # p(w,c)/p(w)*p(c)\n",
    "    pmi_mat = np.log2(mat_p)\n",
    "    \n",
    "    # assume log(0) = 0\n",
    "    np.place(pmi_mat, pmi_mat==-np.inf, 0)\n",
    "    \n",
    "    return pmi_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PMI to the co-occurence matrix computed above with `context_size=1`. What is the PMI between the words \"the\" and \"end\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "pmi_c1 = pmi(co_matrix_c1)\n",
    "# pmi_c2 = pmi(co_matrix_c2)\n",
    "# pmi_c3 = pmi(co_matrix_c3)\n",
    "# pmi_c4 = pmi(co_matrix_c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI of the words the and end is : 2.3773891645927585\n"
     ]
    }
   ],
   "source": [
    "print (f'PMI of the words the and end is : {pmi_c1[token2id_c1[\"the\"], token2id_c1[\"end\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. We also consider an extension of PMI, positive PMI (PPMI), that maps all negative PMI values to 0.0 ([Levy and Goldberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)). \n",
    "Write `ppmi`, which is the same as `pmi` except it applies PPMI instead of PMI (feel free to implement it as an option of `pmi`). What is the PMI of the words \"the\" and \"start\"? The PPMI? [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat, ppmi = False):\n",
    "    \n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI (matrix that holds raw cooccurrence counts)\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"    \n",
    "    \n",
    "    # p(w,d)\n",
    "    mat_p = np.divide(mat, np.sum(mat))\n",
    "    \n",
    "    # p(w), p(d)\n",
    "    p_w, p_d = np.sum(mat_p, 1), np.sum(mat_p, 0)\n",
    "    \n",
    "    # p(w)*p(d)\n",
    "    d = np.outer(p_d, p_w)\n",
    "    \n",
    "    # p(w,d)/p(w)*p(d)\n",
    "    pmi_mat = np.log(np.divide(mat_p, d.T))\n",
    "    \n",
    "    # assume log(0) = 0\n",
    "    np.place(pmi_mat, pmi_mat==-np.inf, 0)\n",
    "    \n",
    "    if ppmi:\n",
    "        np.place(pmi_mat, pmi_mat<0, 0)\n",
    "    \n",
    "    return pmi_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "ppmi_c1 = pmi(co_matrix_c1, ppmi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PMI of the words the and start is 0.25721719834754625\n",
      "The PPMI of the words the and start is 0.17828937582612991\n"
     ]
    }
   ],
   "source": [
    "print (f'The PMI of the words the and start is {pmi_c1[token2id_c1[\"the\"], token2id_c1[\"start\"]]}')\n",
    "print (f'The PPMI of the words the and start is {ppmi_c1[token2id_c1[\"the\"], token2id_c1[\"start\"]]}')\n",
    "# ppmi_c1[token2id_c1[\"the\"], token2id_c1[\"start\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyzing PMI [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweight Matrix\n",
    "\n",
    "__a__. Consider the matrix `np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])`. Reweight this matrix using `ppmi`. \n",
    "\n",
    "    I. What is the value obtained for cell `[0,0]`, and \n",
    "    II. (ii) give a brief description for what is likely problematic about this value. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derin/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "ppmi_mat = pmi(mat=mat, ppmi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.60893804, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.28788209],\n",
       "       [0.22289371, 0.51107566, 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value on the cell [0,0]: 1.6089380373924496. This is largest cell in the matrix and greater than 1 as opposed to all other cells.\n"
     ]
    }
   ],
   "source": [
    "print (f'The value on the cell [0,0]: {ppmi_mat[0,0]}.\\\n",
    " This is largest cell in the matrix and greater than 1 as opposed to all other cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see on the cell right above, the problematic value is 1.608938 on the cell [0,0]. This cell has a much higher value because PMI is __biased toward rare occurrences__, and weighting the matrix using it cuases that value to inflate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the Problematic Value\n",
    "__b__. Give a suggestion for dealing with the problematic value and explain why it deals with this. Demonstrate your suggestion empirically [10 pts]\n",
    "\n",
    "__Answer__\n",
    "The possible solutions are:\n",
    " - Assigning a __minimum probability__ to context words. For example raising the 0 values in the P(c) vector to some value between 0 and 1.\n",
    " - Directly __adding values to the raw co-occurrence matrix__. This should have the same smoothing effect on the weighted matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI for Word-Word Co-occurence Matrix\n",
    "__c__. Consider starting with a word-word co-occurence matrix and apply PMI to this matrix. \n",
    "\n",
    "        I. Which of the following describe the resulting vectors: sparse, dense, high-dimensional, low-dimensional\n",
    "         \n",
    "        II. If you wanted the opposite style of representation, what could you do? [5 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "__I.__ When we start with a word-word co-occurrence matrix, the resulting vectors will be __sparse__ and __high-dimensional__ (depending on the vocabulary size). \n",
    "\n",
    "__II.__ Some ways to achieve dense, low-dimensional vectors:\n",
    " - Using Singular Value Decomposition (SVD) will help decrease the dimensionality\n",
    " - Using Brown clusters to decrease sparsity\n",
    " - Updating the vector representations directly with a neural network-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Word Analogy Evaluation [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word analogies provide another kind of evaluation for distributed representations. Here, we are given three vectors A, B, and C, in the relationship\n",
    "\n",
    "_A is to B as C is to __ _\n",
    "\n",
    "and asked to identify the fourth that completes the analogy. These analogies are by and large substantially easier than the classic brain-teaser analogies that used to appear on tests like the SAT, but it's still an interesting, demanding\n",
    "task. \n",
    "\n",
    "The core idea is that we make predictions by creating the vector\n",
    "\n",
    "$$(B - A) + C$$ \n",
    "\n",
    "and then ranking all vectors based on their distance from this new vector, choosing the closest as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy Completion\n",
    "__a__. Implement the function `analogy_completion`. [9 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_completion(a, b, c, mat,\n",
    "                       token2id, id2token,\n",
    "                       remove_original=False,\n",
    "                       ppmi=False,\n",
    "                       similarity=\"cosine\"):\n",
    "    \"\"\"Compute ? in \n",
    "    a is to b as c is to ? \n",
    "    as the closest to (b-a) + c in terms of cosine distance\n",
    "    \n",
    "    a, b, & c are tokens (string)\n",
    "    \n",
    "    similarity: 'cosine' or 'euclidean'\n",
    "    \"\"\"\n",
    "    \n",
    "    if ppmi:\n",
    "        pmi_mat = pmi(mat, ppmi=True)\n",
    "    else:\n",
    "        pmi_mat = mat\n",
    "    \n",
    "    index_a, index_b, index_c = token2id[a], token2id[b], token2id[c]\n",
    "    \n",
    "    # obtain vectors from pmi-normalized cooccurrence matrix\n",
    "    v_a = pmi_mat[index_a]\n",
    "    v_b = pmi_mat[index_b]\n",
    "    v_c = pmi_mat[index_c]\n",
    "    \n",
    "    # d: b - a + c \n",
    "    v_n = v_b - v_a + v_c\n",
    "    \n",
    "    cs = []\n",
    "    \n",
    "    if similarity == \"cosine\":\n",
    "        for i in range(pmi_mat.shape[0]):\n",
    "            cs.append(cosine_similarity(v_n, pmi_mat[i]))\n",
    "        srt = np.argsort(cs)\n",
    "    \n",
    "    elif similarity == \"euclidean\":\n",
    "        for i in range(pmi_mat.shape[0]):\n",
    "            cs.append(euclidean(v_n, pmi_mat[i]))\n",
    "        srt = np.argsort((-1)*np.array(cs))\n",
    "    \n",
    "    if remove_original:\n",
    "        mask = [x not in [index_a, index_b, index_c] for x in srt]\n",
    "        srt = srt[mask]\n",
    "        ix = srt[0]\n",
    "        d = pmi_mat[ix]\n",
    "    else:\n",
    "        ix = srt[0]\n",
    "        d = pmi_mat[ix]\n",
    "    \n",
    "    return d, ix, id2token[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"theater\", \"art\", \"football\", \n",
    "                                  co_matrix_c2, token2id_c2, id2token_c2, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lobby'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "__b__. Our simple word embeddings likely won't perform well on this task. Let's instead look at some high quality pretrained word embeddings. Write code to load 300-dimensional [GloVe word embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) trained on 840B tokens. Each line of the file is formatted as a word followed by 300 floats that make up its corresponding word embedding (all space delimited). The entries of GloVe word embeddings are not counts, but instead are learned via machine learning. Use your `analogy_completion` code to complete the following analogies using the GloVe word embeddings. [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Beijing\" is to \"China\" as \"Paris\" is to ?\n",
    "- \"gold\" is to \"first\" as \"silver\" is to ?\n",
    "- \"Italian\" is to \"mozzarella\" as \"American\" is to ?\n",
    "- \"research\" is to \"fun\" as \"engineering\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    f = open(fname,\"r\")\n",
    "    glove = {}\n",
    "    for line in f:\n",
    "        s = line.split(\" \")\n",
    "        word = s[0]\n",
    "        embedding = np.array([float(val) for val in s[1:]])\n",
    "        glove[word] = embedding\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = load_vectors(\"data/glove_vec/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_glove(embed_dict):\n",
    "    keys = [*embed_dict.keys()]\n",
    "    token2id = {}\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        token2id[keys[i]] = i\n",
    "        \n",
    "    return token2id\n",
    "\n",
    "def id2token_glove(embed_dict, token2id):\n",
    "    keys = [*token2id.keys()]\n",
    "    id2token = {}\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        id2token[i] = keys[i]\n",
    "        \n",
    "    return id2token\n",
    "\n",
    "def glove_mat(embed_dict, id2token):\n",
    "    edge = len([*embed_dict.keys()])\n",
    "    mat = np.zeros((edge, 300))\n",
    "    \n",
    "    for i in range(edge):\n",
    "        mat[i] = embed_dict[id2token[i]]\n",
    "        \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idGlove = token2id_glove(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idGlove[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tokenGlove = id2token_glove(glove, token2idGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tokenGlove[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveMat = glove_mat(glove, id2tokenGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(gloveMat[3] == glove[\"and\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Beijing\" is to \"China\" as \"Paris\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"Beijing\", \"China\", \"Paris\", \n",
    "                                  gloveMat, token2idGlove, id2tokenGlove, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'most-excellent'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"Beijing\", \"China\", \"Paris\", \n",
    "                                  gloveMat, token2idGlove, id2tokenGlove, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extra-person'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"gold\" is to \"first\" as \"silver\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"gold\", \"first\", \"silver\", \n",
    "                                  gloveMat, token2idGlove, id2tokenGlove, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'News-ID'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Italian\" is to \"mozzarella\" as \"American\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"Italian\", \"mozzarella\", \"American\", \n",
    "                                  gloveMat, token2idGlove, id2tokenGlove, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrightOnly'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"research\" is to \"fun\" as \"engineering\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, ix, token = analogy_completion(\"research\", \"fun\", \"engineering\", \n",
    "                                  gloveMat, token2idGlove, id2tokenGlove, \n",
    "                                  remove_original=True, ppmi=False, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insuranceholistic'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate GloVe\n",
    "c. Let's get a more quantitative, aggregate sense of the quality of GloVe embeddings. Load the analogies from `gram6-nationality-adjective.txt` and evaluate GloVe embeddings. Report the mean reciprocal rank of the correct answer (the last word on each line) for each analogy. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogies(fname):\n",
    "    f = open(fname,\"r\")\n",
    "    analogies = {}\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        s = line.replace(\"\\n\",\"\").split(\" \")\n",
    "        analogies[c] = [s[0], s[1], s[2], s[3]]\n",
    "        c += 1\n",
    "    return pd.DataFrame(analogies).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_df = pd.DataFrame(load_analogies(\"data/gram6-nationality-adjective.txt\"))\n",
    "analogy_df.columns = [\"country1\", \"adj1\", \"country2\", \"adj2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country1</th>\n",
       "      <th>adj1</th>\n",
       "      <th>country2</th>\n",
       "      <th>adj2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>argentina</td>\n",
       "      <td>argentinean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>australia</td>\n",
       "      <td>australian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albania</td>\n",
       "      <td>albanian</td>\n",
       "      <td>austria</td>\n",
       "      <td>austrian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country1      adj1   country2         adj2\n",
       "0  albania  albanian  argentina  argentinean\n",
       "1  albania  albanian  australia   australian\n",
       "2  albania  albanian    austria     austrian"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196016, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloveMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_glove = [norm(x) for x in gloveMat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_evaluation(glove_vecs,\n",
    "                       token2id,\n",
    "                       verbose=False,\n",
    "                       distfunc=\"cosine\",\n",
    "                       fname=\"data/gram6-nationality-adjective.txt\",\n",
    "                       first=180):\n",
    "    \n",
    "    \"\"\"Basic analogies evaluation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    glove_vecs : 2d np.array\n",
    "                 The VSM being evaluated.\n",
    "                 \n",
    "    fname : str, 'data/gram6-nationality-adjective.txt'\n",
    "        \n",
    "    token2id : token2id vocab for glove matrix.\n",
    "        \n",
    "    distfunc : cosine by default, other options not yet implemented\n",
    "    \n",
    "    first : int, pass n < num_analogies to evaluate first n rows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (float, float)\n",
    "        MRR: The first is the mean reciprocal rank of the predictions and \n",
    "        ACC: the second is the accuracy of the predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rr = []\n",
    "    similarity = {}\n",
    "    preds = []\n",
    "    pred_vecs = []\n",
    "    \n",
    "    test_df = load_analogies(fname).iloc[:first]\n",
    "    test_df.columns = [\"country1\", \"adj1\", \"country2\", \"adj2\"]\n",
    "    targets = [token2id[x] for x in [*test_df[\"adj2\"]]]\n",
    "    \n",
    "    for i in range(test_df.shape[0]):\n",
    "        a1 = glove_vecs[token2id[test_df.iloc[i][\"adj1\"]]]\n",
    "        c1 = glove_vecs[token2id[test_df.iloc[i][\"country1\"]]]\n",
    "        c2 = glove_vecs[token2id[test_df.iloc[i][\"country2\"]]]\n",
    "        target_index = token2id[test_df.iloc[i][\"adj2\"]]\n",
    "        pred_v = a1 - c1 + c2 # (b - a) + c\n",
    "        norm_pred = norm(pred_v)\n",
    "        dots = [np.dot(pred_v, glove_vecs[g]) for g in range(glove_vecs.shape[0])]\n",
    "        similarity[i] = [dots[g]/(norm_pred*norm_glove[g]) for g in range(len(norm_glove))]\n",
    "        original_index = np.array([token2id[test_df.iloc[i][\"adj1\"]],\n",
    "                                   token2id[test_df.iloc[i][\"country1\"]],\n",
    "                                   token2id[test_df.iloc[i][\"country2\"]]])\n",
    "        \n",
    "        rank = [*np.arange(glove_vecs.shape[0])[np.argsort((-1)*np.array(similarity[i]))]]\n",
    "        # remove original vector indices from ranking\n",
    "        for r in original_index:\n",
    "            rank.remove(r)\n",
    "\n",
    "        reciprocal = 1/([*rank].index(target_index)+1)\n",
    "        print (reciprocal)\n",
    "        rr.append(reciprocal)\n",
    "        p = rank[0]\n",
    "        print (\"pred = \", p)\n",
    "        print (\"target = \", targets[i])\n",
    "        preds.append(p)\n",
    "    \n",
    "    mrr = np.mean(rr)\n",
    "    acc = np.mean([int(preds[i]==targets[i]) for i in range(len(preds))])\n",
    "    \n",
    "    return mrr, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013888888888888888\n",
      "pred =  239769\n",
      "target =  672388\n",
      "1.0\n",
      "pred =  29444\n",
      "target =  29444\n",
      "0.14285714285714285\n",
      "pred =  82314\n",
      "target =  116803\n",
      "0.007936507936507936\n",
      "pred =  90378\n",
      "target =  1697246\n",
      "1.0\n",
      "pred =  31502\n",
      "target =  31502\n",
      "1.0\n",
      "pred =  113264\n",
      "target =  113264\n",
      "1.0\n",
      "pred =  215369\n",
      "target =  215369\n",
      "0.018867924528301886\n",
      "pred =  14121\n",
      "target =  161738\n",
      "1.0\n",
      "pred =  10980\n",
      "target =  10980\n",
      "1.0\n",
      "pred =  135420\n",
      "target =  135420\n",
      "1.0\n",
      "pred =  165327\n",
      "target =  165327\n",
      "1.0\n",
      "pred =  56022\n",
      "target =  56022\n",
      "1.0\n",
      "pred =  50500\n",
      "target =  50500\n",
      "0.3333333333333333\n",
      "pred =  50087\n",
      "target =  6001\n",
      "1.0\n",
      "pred =  8514\n",
      "target =  8514\n",
      "1.0\n",
      "pred =  13687\n",
      "target =  13687\n",
      "1.0\n",
      "pred =  22145\n",
      "target =  22145\n",
      "1.0\n",
      "pred =  157153\n",
      "target =  157153\n",
      "1.0\n",
      "pred =  9502\n",
      "target =  9502\n",
      "1.0\n",
      "pred =  25982\n",
      "target =  25982\n",
      "1.0\n",
      "pred =  79816\n",
      "target =  79816\n",
      "1.0\n",
      "pred =  16373\n",
      "target =  16373\n",
      "1.0\n",
      "pred =  11018\n",
      "target =  11018\n",
      "1.0\n",
      "pred =  25489\n",
      "target =  25489\n",
      "1.0\n",
      "pred =  288044\n",
      "target =  288044\n",
      "0.017857142857142856\n",
      "pred =  165327\n",
      "target =  95703\n",
      "1.0\n",
      "pred =  18971\n",
      "target =  18971\n",
      "0.1111111111111111\n",
      "pred =  86253\n",
      "target =  1380683\n",
      "1.0\n",
      "pred =  28190\n",
      "target =  28190\n",
      "1.0\n",
      "pred =  71967\n",
      "target =  71967\n",
      "1.0\n",
      "pred =  137477\n",
      "target =  137477\n",
      "0.007142857142857143\n",
      "pred =  140889\n",
      "target =  11117\n",
      "1.0\n",
      "pred =  52923\n",
      "target =  52923\n",
      "1.0\n",
      "pred =  13601\n",
      "target =  13601\n",
      "0.14285714285714285\n",
      "pred =  194352\n",
      "target =  592660\n",
      "1.0\n",
      "pred =  14107\n",
      "target =  14107\n",
      "1.0\n",
      "pred =  39008\n",
      "target =  39008\n",
      "0.25\n",
      "pred =  138086\n",
      "target =  25548\n",
      "1.0\n",
      "pred =  24395\n",
      "target =  24395\n",
      "0.5\n",
      "pred =  141371\n",
      "target =  29444\n",
      "1.0\n",
      "pred =  116803\n",
      "target =  116803\n",
      "0.0017211703958691911\n",
      "pred =  1913501\n",
      "target =  1697246\n",
      "0.3333333333333333\n",
      "pred =  235044\n",
      "target =  31502\n",
      "0.3333333333333333\n",
      "pred =  592660\n",
      "target =  113264\n",
      "1.0\n",
      "pred =  215369\n",
      "target =  215369\n",
      "0.017543859649122806\n",
      "pred =  112785\n",
      "target =  161738\n",
      "0.3333333333333333\n",
      "pred =  330363\n",
      "target =  10980\n",
      "1.0\n",
      "pred =  135420\n",
      "target =  135420\n",
      "0.2\n",
      "pred =  416846\n",
      "target =  165327\n",
      "0.5\n",
      "pred =  952019\n",
      "target =  56022\n",
      "1.0\n",
      "pred =  50500\n",
      "target =  50500\n",
      "0.00011309658448314861\n",
      "pred =  139705\n",
      "target =  6001\n",
      "0.1\n",
      "pred =  285733\n",
      "target =  8514\n",
      "0.5\n",
      "pred =  116803\n",
      "target =  13687\n",
      "0.1\n",
      "pred =  452781\n",
      "target =  22145\n",
      "0.5\n",
      "pred =  454551\n",
      "target =  157153\n",
      "1.0\n",
      "pred =  9502\n",
      "target =  9502\n",
      "0.2\n",
      "pred =  505223\n",
      "target =  25982\n",
      "1.0\n",
      "pred =  79816\n",
      "target =  79816\n",
      "0.25\n",
      "pred =  295263\n",
      "target =  16373\n",
      "1.0\n",
      "pred =  11018\n",
      "target =  11018\n",
      "0.3333333333333333\n",
      "pred =  791069\n",
      "target =  25489\n",
      "1.0\n",
      "pred =  288044\n",
      "target =  288044\n",
      "1.3291332721932027e-05\n",
      "pred =  452781\n",
      "target =  95703\n",
      "1.0\n",
      "pred =  18971\n",
      "target =  18971\n",
      "0.3333333333333333\n",
      "pred =  974235\n",
      "target =  1380683\n",
      "0.011627906976744186\n",
      "pred =  592660\n",
      "target =  28190\n",
      "0.5\n",
      "pred =  725018\n",
      "target =  71967\n",
      "1.0\n",
      "pred =  137477\n",
      "target =  137477\n",
      "1.141826236883271e-05\n",
      "pred =  592660\n",
      "target =  11117\n",
      "0.0196078431372549\n",
      "pred =  153008\n",
      "target =  52923\n",
      "0.06666666666666667\n",
      "pred =  59657\n",
      "target =  13601\n",
      "1.0\n",
      "pred =  592660\n",
      "target =  592660\n",
      "0.005076142131979695\n",
      "pred =  513736\n",
      "target =  14107\n",
      "0.125\n",
      "pred =  402387\n",
      "target =  39008\n",
      "0.0037313432835820895\n",
      "pred =  592660\n",
      "target =  25548\n",
      "0.05\n",
      "pred =  121507\n",
      "target =  24395\n",
      "0.3333333333333333\n",
      "pred =  265596\n",
      "target =  90378\n",
      "0.5\n",
      "pred =  68904\n",
      "target =  116803\n",
      "0.008130081300813009\n",
      "pred =  90378\n",
      "target =  1697246\n",
      "1.0\n",
      "pred =  31502\n",
      "target =  31502\n",
      "1.0\n",
      "pred =  113264\n",
      "target =  113264\n",
      "1.0\n",
      "pred =  215369\n",
      "target =  215369\n",
      "0.0625\n",
      "pred =  14121\n",
      "target =  161738\n",
      "1.0\n",
      "pred =  10980\n",
      "target =  10980\n",
      "1.0\n",
      "pred =  135420\n",
      "target =  135420\n",
      "1.0\n",
      "pred =  165327\n",
      "target =  165327\n",
      "0.25\n",
      "pred =  51026\n",
      "target =  56022\n",
      "1.0\n",
      "pred =  50500\n",
      "target =  50500\n",
      "0.045454545454545456\n",
      "pred =  17615\n",
      "target =  6001\n",
      "1.0\n",
      "pred =  8514\n",
      "target =  8514\n",
      "1.0\n",
      "pred =  13687\n",
      "target =  13687\n",
      "1.0\n",
      "pred =  22145\n",
      "target =  22145\n",
      "1.0\n",
      "pred =  157153\n",
      "target =  157153\n",
      "1.0\n",
      "pred =  9502\n",
      "target =  9502\n",
      "1.0\n",
      "pred =  25982\n",
      "target =  25982\n",
      "1.0\n",
      "pred =  79816\n",
      "target =  79816\n",
      "1.0\n",
      "pred =  16373\n",
      "target =  16373\n",
      "1.0\n",
      "pred =  11018\n",
      "target =  11018\n",
      "1.0\n",
      "pred =  25489\n",
      "target =  25489\n",
      "1.0\n",
      "pred =  288044\n",
      "target =  288044\n",
      "0.034482758620689655\n",
      "pred =  165327\n",
      "target =  95703\n",
      "1.0\n",
      "pred =  18971\n",
      "target =  18971\n",
      "0.05263157894736842\n",
      "pred =  239175\n",
      "target =  1380683\n",
      "0.2\n",
      "pred =  68904\n",
      "target =  28190\n",
      "0.3333333333333333\n",
      "pred =  51026\n",
      "target =  71967\n",
      "0.5\n",
      "pred =  127770\n",
      "target =  137477\n",
      "0.00273224043715847\n",
      "pred =  106894\n",
      "target =  11117\n",
      "0.5\n",
      "pred =  147463\n",
      "target =  52923\n",
      "1.0\n",
      "pred =  13601\n",
      "target =  13601\n",
      "0.5\n",
      "pred =  194352\n",
      "target =  592660\n",
      "1.0\n",
      "pred =  14107\n",
      "target =  14107\n",
      "1.0\n",
      "pred =  39008\n",
      "target =  39008\n",
      "0.09090909090909091\n",
      "pred =  68904\n",
      "target =  25548\n",
      "1.0\n",
      "pred =  24395\n",
      "target =  24395\n",
      "1.0\n",
      "pred =  90378\n",
      "target =  90378\n",
      "1.0\n",
      "pred =  165397\n",
      "target =  165397\n",
      "0.00980392156862745\n",
      "pred =  90378\n",
      "target =  1697246\n",
      "1.0\n",
      "pred =  31502\n",
      "target =  31502\n",
      "1.0\n",
      "pred =  113264\n",
      "target =  113264\n",
      "1.0\n",
      "pred =  215369\n",
      "target =  215369\n",
      "0.030303030303030304\n",
      "pred =  14121\n",
      "target =  161738\n",
      "1.0\n",
      "pred =  10980\n",
      "target =  10980\n",
      "1.0\n",
      "pred =  135420\n",
      "target =  135420\n",
      "1.0\n",
      "pred =  165327\n",
      "target =  165327\n",
      "1.0\n",
      "pred =  56022\n",
      "target =  56022\n",
      "1.0\n",
      "pred =  50500\n",
      "target =  50500\n",
      "0.05555555555555555\n",
      "pred =  17615\n",
      "target =  6001\n",
      "1.0\n",
      "pred =  8514\n",
      "target =  8514\n",
      "1.0\n",
      "pred =  13687\n",
      "target =  13687\n",
      "1.0\n",
      "pred =  22145\n",
      "target =  22145\n",
      "1.0\n",
      "pred =  157153\n",
      "target =  157153\n",
      "1.0\n",
      "pred =  9502\n",
      "target =  9502\n",
      "1.0\n",
      "pred =  25982\n",
      "target =  25982\n",
      "1.0\n",
      "pred =  79816\n",
      "target =  79816\n",
      "1.0\n",
      "pred =  16373\n",
      "target =  16373\n",
      "1.0\n",
      "pred =  11018\n",
      "target =  11018\n",
      "1.0\n",
      "pred =  25489\n",
      "target =  25489\n",
      "1.0\n",
      "pred =  288044\n",
      "target =  288044\n",
      "0.017857142857142856\n",
      "pred =  165327\n",
      "target =  95703\n",
      "1.0\n",
      "pred =  18971\n",
      "target =  18971\n",
      "0.16666666666666666\n",
      "pred =  974235\n",
      "target =  1380683\n",
      "1.0\n",
      "pred =  28190\n",
      "target =  28190\n",
      "1.0\n",
      "pred =  71967\n",
      "target =  71967\n",
      "1.0\n",
      "pred =  137477\n",
      "target =  137477\n",
      "0.007874015748031496\n",
      "pred =  56053\n",
      "target =  11117\n",
      "1.0\n",
      "pred =  52923\n",
      "target =  52923\n",
      "1.0\n",
      "pred =  13601\n",
      "target =  13601\n",
      "0.5\n",
      "pred =  194352\n",
      "target =  592660\n",
      "1.0\n",
      "pred =  14107\n",
      "target =  14107\n",
      "1.0\n",
      "pred =  39008\n",
      "target =  39008\n",
      "1.0\n",
      "pred =  25548\n",
      "target =  25548\n",
      "1.0\n",
      "pred =  24395\n",
      "target =  24395\n",
      "1.0\n",
      "pred =  90378\n",
      "target =  90378\n",
      "1.0\n",
      "pred =  165397\n",
      "target =  165397\n",
      "0.14285714285714285\n",
      "pred =  239769\n",
      "target =  672388\n",
      "1.0\n",
      "pred =  31502\n",
      "target =  31502\n",
      "0.3333333333333333\n",
      "pred =  2004271\n",
      "target =  113264\n",
      "1.0\n",
      "pred =  215369\n",
      "target =  215369\n",
      "0.011494252873563218\n",
      "pred =  14121\n",
      "target =  161738\n",
      "0.5\n",
      "pred =  27287\n",
      "target =  10980\n",
      "0.3333333333333333\n",
      "pred =  212183\n",
      "target =  135420\n",
      "1.0\n",
      "pred =  165327\n",
      "target =  165327\n",
      "0.03125\n",
      "pred =  1413579\n",
      "target =  56022\n",
      "1.0\n",
      "pred =  50500\n",
      "target =  50500\n",
      "0.005076142131979695\n",
      "pred =  81918\n",
      "target =  6001\n",
      "1.0\n",
      "pred =  8514\n",
      "target =  8514\n",
      "1.0\n",
      "pred =  13687\n",
      "target =  13687\n",
      "1.0\n",
      "pred =  22145\n",
      "target =  22145\n",
      "1.0\n",
      "pred =  157153\n",
      "target =  157153\n",
      "0.25\n",
      "pred =  60957\n",
      "target =  9502\n",
      "0.08333333333333333\n",
      "pred =  77542\n",
      "target =  25982\n",
      "1.0\n",
      "pred =  79816\n",
      "target =  79816\n",
      "1.0\n",
      "pred =  16373\n",
      "target =  16373\n",
      "1.0\n",
      "pred =  11018\n",
      "target =  11018\n",
      "1.0\n",
      "pred =  25489\n",
      "target =  25489\n",
      "0.3333333333333333\n",
      "pred =  733603\n",
      "target =  288044\n",
      "3.803727653100038e-05\n",
      "pred =  497678\n",
      "target =  95703\n",
      "1.0\n",
      "pred =  18971\n",
      "target =  18971\n",
      "0.022727272727272728\n",
      "pred =  974235\n",
      "target =  1380683\n"
     ]
    }
   ],
   "source": [
    "mrr, acc = analogy_evaluation(gloveMat, \n",
    "                              token2idGlove,\n",
    "                              fname=\"data/gram6-nationality-adjective.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.675622082960195, 0.6055555555555555)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9391509433962264, defaultdict(int, {True: 97, False: 9}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_evaluation(glove_vecs, \"gram6-nationality-adjective.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
